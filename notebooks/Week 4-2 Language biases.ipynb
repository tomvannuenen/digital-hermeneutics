{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: block; width: 100%; height: 120px;\">\n",
    "\n",
    "<p style=\"float: left;\">\n",
    "    <span style=\"font-weight: bold; line-height: 24px; font-size: 16px;\">\n",
    "        DIGHUM160 - Critical Digital Humanities\n",
    "        <br />\n",
    "        Digital Hermeneutics\n",
    "    </span>\n",
    "    <br >\n",
    "    <span style=\"line-height: 22x; font-size: 14x; margin-top: 10px;\">\n",
    "        Week 4-2: Biases in word embeddings<br />\n",
    "        Created by Xavier Ferrer (xavier.ferrer.aran@kcl.ac.uk) and Tom van Nuenen (tom.van_nuenen@kcl.ac.uk)<br />\n",
    "    </span>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtIBMFnNrQlL"
   },
   "source": [
    "## Introduction\n",
    "Language carries implicit biases, functioning both as a reflection and a perpetuation of stereotypes that people carry with them. Using Natural Language Processing tools, we can trace these biases in the many language datasets to be found online.\n",
    "\n",
    "One way to discover language biases is done using word embeddings. In order to do so, we first need to postulate concepts such as \"male\" or \"female\", both of which include a number of word vectors. Using these so-called *target concepts*, we can then compute relative similarities of other word vectors – particularly, words that act as evaluative attributes such as \"strong\" and \"sensitive\". \n",
    "\n",
    "These words can be categorised through clustering algorithms and labeled through a semantic analysis system into more general (conceptual) biases, yielding a broad picture of the biases present in a discourse community.\n",
    "\n",
    "See https://xfold.github.io/WE-GenderBiasVisualisationWeb/ for a web demo\n",
    "and https://github.com/xfold/LanguageBiasesInReddit for the full repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TShMBIgV23jf"
   },
   "source": [
    "## Training a WE model\n",
    "\n",
    "First, we need to train our Word Embeddings model. We create a function that takes in a CSV file and applies Gensim's `simple_preprocess` method on the \"body\" column.It also lemmatizes the data if we want, and finally creates a Word2Vec model with parameters we can feed into the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "VZ6k3qaS0CbE"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "# Current notebook only works with Gensim v3 - e.g. !pip install gensim==3.8.1\n",
    "import gensim \n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def train_model(csv_document, csv_comment_column='body', outputname='output_model', window = 4, minf=10, epochs=100, ndim=100, lemmatiseFirst = False):\n",
    "    '''\n",
    "    Load the documents from document_l, a list of sentences, and train a WE model with specified\n",
    "    minf, epochs and ndims. where:\n",
    "    csv_document : csv document containing all information, where each comment is on a different row\n",
    "    csv_comment_column : name of the column taht contains the text we want to process\n",
    "    outputname : output path of the resulting model\n",
    "    \n",
    "    returns\n",
    "    path of the trained models\n",
    "    '''\n",
    "    \n",
    "    def preprocess_csv(path, column = 'body'):\n",
    "        df_com = pd.read_csv(path, lineterminator='\\n')\n",
    "\n",
    "        documents = []\n",
    "        for i, row in enumerate(df_com[column]):\n",
    "            if i%500000 == 0:\n",
    "                print('\\t...processing line {}'.format(i))\n",
    "            try:\n",
    "                pp = gensim.utils.simple_preprocess (row)\n",
    "                if(lemmatiseFirst == True):\n",
    "                    pp = [wordnet_lemmatizer.lemmatize(w, pos=\"n\") for w in pp]\n",
    "                documents.append(pp)\n",
    "            except:\n",
    "                print('\\terror with row {}'.format(row))\n",
    "\n",
    "        logging.info (\"Done reading and preprocessing data file {} \".format(path))\n",
    "        return documents\n",
    "\n",
    "    def train_WE_model(documents, outputfile, ndim, window, minfreq, epochss):\n",
    "        '''\n",
    "        size\n",
    "        The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller\n",
    "        value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me.\n",
    "\n",
    "        window\n",
    "        The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum \n",
    "        window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a \n",
    "        smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too \n",
    "        much, as long as its a decent sized window.\n",
    "\n",
    "        min_count\n",
    "        Minimium frequency count of words. The model would ignore words that do not statisfy the min_count. Extremely infrequent words are \n",
    "        usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "        workers\n",
    "        How many threads to use behind the scenes?\n",
    "        '''\n",
    "        starttime = time.time()\n",
    "        print('->->Starting training model {} with dimensions:{}, minf:{}, epochs:{}'.format(outputfile,ndim, minfreq, epochss))\n",
    "        model = gensim.models.Word2Vec (documents, size=ndim, window=window, min_count=minfreq, workers=5)\n",
    "        model.train(documents,total_examples=len(documents),epochs=epochss)\n",
    "        model.save(outputfile)\n",
    "        print('->-> Model saved in {}'.format(outputfile))\n",
    "    \n",
    "    print('->Starting with {} [{}], output {}, window {}, minf {}, epochs {}, ndim {}'.format(csv_document, \n",
    "                                                                                       csv_comment_column,\n",
    "                                                                                       outputname, window, minf, epochs, ndim))\n",
    "    docs = preprocess_csv(csv_document, csv_comment_column)\n",
    "    starttime = time.time()\n",
    "    ofile = outputname\n",
    "    print('-> Output will be saved in {}'.format(ofile))\n",
    "    train_WE_model(docs, ofile, ndim, window, minf, epochs)\n",
    "    print('-> Model creation ended in {} seconds'.format(time.time()-starttime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9NtlPTtQucJ"
   },
   "source": [
    "This function has been created to run over different CSVs and using different parameters (like we did with our topic models). Below, we create a `training_setup` dictionary that can include multiple CSV files and parameters. This makes it a bit easier to replicate the process. For now, we've entered one CSV file: the one we have loaded. We will save the output of our function – the Word Embeddings model – in a file as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "executionInfo": {
     "elapsed": 4637,
     "status": "error",
     "timestamp": 1620740203059,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "VgJPLcKjrQlV",
    "outputId": "8a4f3f95-5de6-4081-ae7d-6a0eb2827fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->Starting with data/TRP-comments.csv [body], output trp_w4_f10_e100_d200.model, window 4, minf 2, epochs 100, ndim 100\n",
      "\t...processing line 0\n",
      "-> Output will be saved in trp_w4_f10_e100_d200.model\n",
      "->->Starting training model trp_w4_f10_e100_d200.model with dimensions:100, minf:2, epochs:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->-> Model saved in trp_w4_f10_e100_d200.model\n",
      "-> Model creation ended in 36.7374529838562 seconds\n"
     ]
    }
   ],
   "source": [
    "training_setup = [\n",
    "    {'csvfile': \"data/TRP-comments.csv\", 'output_file': 'trp_w4_f10_e100_d200.model', 'w':4, 'minf': 2, 'epochs':100 ,'ndim':100}\n",
    "]\n",
    "\n",
    "for setup in training_setup:\n",
    "        train_model(setup['csvfile'], \n",
    "        outputname = setup['output_file'],\n",
    "        window = setup['w'],\n",
    "        minf = setup['minf'],\n",
    "        epochs = setup['epochs'],\n",
    "        ndim = setup['ndim']\n",
    "        )\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSbUVbC-rQlY"
   },
   "source": [
    "## Load Model and get biased words\n",
    "\n",
    "We now run our method of finding biased words towards our target sets.\n",
    "\n",
    "Given a vocabulary and two sets of target words (such as, in this case, those for *women* and *men*, we rank the words from least to most biased. As such, we obtain two ordered lists of the most biased words towards each target set, obtaining an overall view of the bias distribution in that particular community with respect to those two target sets. \n",
    "\n",
    "Here's what happening in the next block of code:\n",
    "- We calculate the centroid of a target set by averaging the embedding vectors in our target set (e.g. the vectors for `he, son, his, him, father, male` for our target concept `male`);\n",
    "- We calculate the cosine similarity between the vectors for all words in our vocabulary as compared to our two centroids (we also apply POS-filtering to only work with parts of speech we expect to be relevant);\n",
    "- We use a threshold based on standard deviation to determine how severe a bias needs to be before we include it;\n",
    "- We rank the words in the vocabulary of our Word Embeddings model based on their bias towards either target concept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3335,
     "status": "ok",
     "timestamp": 1620734129754,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "4AHqdHpSrQlZ",
    "outputId": "dfa016ba-9005-49a8-b5f3-15a58c754ccf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tomvannuenen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/tomvannuenen/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from operator import itemgetter\n",
    "from scipy import spatial\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "import inflect\n",
    "import numpy as np\n",
    "import statistics\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from datetime import datetime\n",
    "import statistics\n",
    "\n",
    "def _calculate_centroid(model, wordlist):\n",
    "    '''\n",
    "    Calculate centroid of the wordlist list of words based on the model embedding vectors\n",
    "    '''\n",
    "    centr = np.zeros( len(model.wv[wordlist[0]]) )\n",
    "    for w in wordlist:\n",
    "        centr += np.array(model.wv[w])\n",
    "    return centr/len(wordlist)\n",
    "\n",
    "def _keep_only_model_words(model, words):\n",
    "    aux = [ word for word in words if word in model.wv.vocab.keys()]\n",
    "    return aux\n",
    "\n",
    "def _get_word_freq(model, word):\n",
    "    if word in model.wv.vocab:\n",
    "        wm = model.wv.vocab[word]\n",
    "        return [word, wm.count, wm.index]\n",
    "    return None\n",
    "\n",
    "def _get_model_min_max_rank(model):\n",
    "    minF = 999999\n",
    "    maxF = -1\n",
    "    for w in model.wv.vocab:\n",
    "        wm = model.wv.vocab[w] #wm.count, wm.index\n",
    "        rank = wm.index\n",
    "        if(minF>rank):\n",
    "            minF = rank\n",
    "        if(maxF<rank):\n",
    "            maxF = rank\n",
    "    return [minF, maxF]\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "def _get_sentiment(word):\n",
    "    return sid.polarity_scores(word)['compound']\n",
    "\n",
    "'''\n",
    "Normalises a value in the positive space\n",
    "'''    \n",
    "def _normalise(val, minF, maxF):\n",
    "    #print(val, minF, maxF)\n",
    "    if(maxF<0 or minF<0 or val<0):\n",
    "        raise Exception('All values should be in the positive space. minf: {}, max: {}, freq: {}'.format(minF, maxF, val))\n",
    "    if(maxF<= minF):\n",
    "        raise Exception('Maximum frequency should be bigger than min frequency. minf: {}, max: {}, freq: {}'.format(minF, maxF, freq))\n",
    "    val -= minF\n",
    "    val = val/(maxF-minF)\n",
    "    return val\n",
    "\n",
    "def _get_cosine_distance(wv1, wv2):\n",
    "    return spatial.distance.cosine(wv1, wv2)\n",
    "\n",
    "def _get_min_max(dict_value):\n",
    "    l = list(dict_value.values())\n",
    "    return [ min(l), max(l)]\n",
    "\n",
    "def _find_stdev_threshold_sal(dwords, stdevs):\n",
    "    '''\n",
    "    dword is an object like {'word':w, 'bias':bias, 'biasW':biasW, 'freq':freq, 'freqW':freqW, 'sal':val, 'wv':wv, 'sent':sent }\n",
    "    stdevs : minimum stdevs for which we want to compute the threshold\n",
    "\n",
    "    returns\n",
    "    outlier_thr : the threshold correpsonding to stdevs considering salience values from the dwrods object list\n",
    "    '''\n",
    "    allsal = []\n",
    "    for obj in dwords:\n",
    "        allsal.append(obj['sal'])\n",
    "    stdev = statistics.stdev(allsal)\n",
    "    outlier_thr = (stdev*stdevs)+sum(allsal)/len(allsal)\n",
    "    return outlier_thr\n",
    "\n",
    "def calculate_biased_words(model, targetset1, targetset2, stdevs, \n",
    "                         acceptedPOS = ['JJ', 'JJS', 'JJR','NN', 'NNS', 'NNP', 'NNPS','VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ' ], \n",
    "                         words = None, force=False):\n",
    "    '''\n",
    "    this function calculates the list of biased words towards targetset1 and taregset2 with salience > than the \n",
    "    specified times (minstdev) of standard deviation.\n",
    "\n",
    "    targetset1 <list of strings> : target set 1\n",
    "    targetset2 <list of strings> : target set 2\n",
    "    minstdev int : Minium threhsold for stdev to select biased words\n",
    "    acceptedPOS <list<str>> : accepted list of POS to consider for the analysis, as defined in NLTK POS tagging lib. \n",
    "                              If None, no POS filtering is applied and all words in the vocab are considered\n",
    "    words list<str> : list of words we want to consider. If None, all words in the vocab are considered\n",
    "    '''\n",
    "    if(model is None):\n",
    "        raise Exception(\"You need to define a model to estimate biased words.\")\n",
    "    if(targetset1 is None or targetset2 is None):\n",
    "        raise Exception(\"Target sets are necessary to estimate biased words.\")\n",
    "    if(stdevs is None):\n",
    "        raise Exception(\"You need to define a minimum threshold for standard deviation to select biased words.\")\n",
    "   \n",
    "    tset1 = _keep_only_model_words(model, targetset1) # remove target set words that do not exist in the model\n",
    "    tset2 = _keep_only_model_words(model, targetset2) # remove target set words that do not exist in the model\n",
    "\n",
    "    # We remove words in the target sets, and also their plurals from the set of interesting words to process.\n",
    "    engine = inflect.engine()\n",
    "    toremove = targetset1 + targetset2 + [engine.plural(w) for w in targetset1] + [engine.plural(w) for w in targetset2]\n",
    "    if(words is None):\n",
    "        words = [w for w in model.wv.vocab.keys() if w not in toremove]\n",
    "\n",
    "    # Calculate centroids \n",
    "    tset1_centroid = _calculate_centroid(model, tset1)\n",
    "    tset2_centroid = _calculate_centroid(model, tset2)\n",
    "    [minR, maxR] = _get_model_min_max_rank(model)\n",
    "\n",
    "    # Get biases for words\n",
    "    biasWF = {}\n",
    "    biasWM = {}\n",
    "    for i, w in enumerate(words):\n",
    "        p = nltk.pos_tag([w])[0][1]\n",
    "        if acceptedPOS is not None and p not in acceptedPOS:\n",
    "            continue\n",
    "        wv = model.wv[w]\n",
    "        diff = _get_cosine_distance(tset2_centroid, wv) - _get_cosine_distance(tset1_centroid, wv)\n",
    "        if(diff>0):\n",
    "            biasWF[w] = diff\n",
    "        else:\n",
    "            biasWM[w] = -1*diff\n",
    "\n",
    "    # Get min and max bias for both target sets, so we can normalise these values later\n",
    "    [minbf, maxbf] = _get_min_max(biasWF)\n",
    "    [minbm, maxbm] = _get_min_max(biasWM)\n",
    "\n",
    "    # Iterate through all 'selected' words\n",
    "    biased1 = []\n",
    "    biased2 = []\n",
    "    for i, w in enumerate(words):\n",
    "        # Print('..Processing ', w)\n",
    "        p = nltk.pos_tag([w])[0][1]\n",
    "        if acceptedPOS is not None and p not in acceptedPOS:\n",
    "            continue\n",
    "        wv = model.wv[w]\n",
    "        # Sentiment\n",
    "        sent = _get_sentiment(w)\n",
    "        # Rank and rank norm\n",
    "        freq = _get_word_freq(model, w)[1]\n",
    "        rank = _get_word_freq(model, w)[2]\n",
    "        rankW = 1-_normalise(rank, minR, maxR) \n",
    "\n",
    "        # Normalise bias\n",
    "        if(w in biasWF):\n",
    "            bias = biasWF[w]\n",
    "            biasW = _normalise(bias, minbf, maxbf)\n",
    "            val = biasW * rankW\n",
    "            biased1.append({'word':w, 'bias':bias, 'biasW':biasW, 'freq':freq, 'rank':rank, 'rankW':rankW, 'sal':val, 'wv':wv.tolist(), 'sent':sent } ) \n",
    "        if(w in biasWM):\n",
    "            bias = biasWM[w]\n",
    "            biasW = _normalise(bias, minbm, maxbm)\n",
    "            val = biasW * rankW\n",
    "            biased2.append({'word':w, 'bias':bias, 'biasW':biasW, 'freq':freq, 'rank':rank, 'rankW':rankW, 'sal':val, 'wv':wv.tolist(), 'sent':sent } ) \n",
    "\n",
    "    # Calculate the salience threshold for both word sets, and select the list of biased words (i.e., which words do we discard?)\n",
    "    stdevs1_thr = _find_stdev_threshold_sal(biased1, stdevs)\n",
    "    stdevs2_thr = _find_stdev_threshold_sal(biased2, stdevs)\n",
    "    # biased1.sort(key=lambda x: x['sal'], reverse=True)\n",
    "    b1_dict = {}\n",
    "    for k in biased1:\n",
    "        if(k['sal']>=stdevs1_thr):\n",
    "            b1_dict[k['word']] = k\n",
    "    # biased2.sort(key=lambda x: x['sal'], reverse=True)\n",
    "    b2_dict = {}\n",
    "    for k in biased2:\n",
    "        if(k['sal']>=stdevs2_thr):\n",
    "            b2_dict[k['word']] = k\n",
    "\n",
    "    #transform centroid tol list so they become serializable\n",
    "    tset1_centroid = tset1_centroid.tolist() \n",
    "    tset2_centroid = tset2_centroid.tolist()\n",
    "    return [b1_dict, b2_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "executionInfo": {
     "elapsed": 1094,
     "status": "error",
     "timestamp": 1620734159039,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuBmxDvW5I_LJfZtwlPqMFD8QGLVPP3skvpkTnuQ=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "T7wSXVuJrQld",
    "outputId": "fd28501f-fcac-4581-e6ba-b92f170e142a"
   },
   "outputs": [],
   "source": [
    "modelpath = \"trp_w4_f10_e100_d200.model\"\n",
    "\n",
    "model = Word2Vec.load(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "p9UI-saw2vIl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('men', 0.8824670314788818),\n",
       " ('people', 0.7178006768226624),\n",
       " ('they', 0.6622591018676758),\n",
       " ('girls', 0.6600404977798462),\n",
       " ('feminists', 0.6574165225028992),\n",
       " ('guys', 0.6305696964263916),\n",
       " ('females', 0.6152883768081665),\n",
       " ('them', 0.5591142177581787),\n",
       " ('alphas', 0.5416093468666077),\n",
       " ('humans', 0.5091690421104431)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get similar words\n",
    "sims = model.wv.most_similar('women', topn=10)  # get other similar words\n",
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aan7eV3SqhkZ"
   },
   "source": [
    "Here we create the two target sets, called `t1` and `t2`. These two lists are the ones you'll want to swap out if you are going to create your own target sets to find biases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "SIzdtQV5rQlh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t1=[\"sister\" , \"female\" , \"woman\" , \"girl\" , \"daughter\" , \"she\" , \"hers\" , \"her\"]\n",
    "t2=[\"brother\" , \"male\" , \"man\" , \"boy\" , \"son\" , \"he\" , \"his\" , \"him\"] \n",
    "\n",
    "[b1, b2] = calculate_biased_words(model, t1, t2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJpg5BDydlYT"
   },
   "source": [
    "Let's print some biases. Here you see the most-biased words towards our target concepts (1 being *women*, 2 being *men*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "executionInfo": {
     "elapsed": 153596,
     "status": "ok",
     "timestamp": 1596264125375,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "qc-9g8BErQll",
    "outputId": "39a3c15d-8f5f-4348-8595-398a0521ebd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased words towards target set 1\n",
      "['fact', 'brain', 'food', 'body', 'apps', 'chances', 'reality', 'text', 'obvious', 'spinning', 'anyone', 'pussy', 'rationalization', 'partner', 'problems', 'everyone', 'hamster', 'lawyer', 'assume', 'conversation', 'issue', 'chick', 'mine', 'moment', 'chicks', 'hubby', 'answer', 'number', 'exclusivity', 'plate', 'ignoring', 'levels', 'okay', 'increasing', 'forget', 'phone', 'hang', 'everybody', 'physical', 'clubs', 'tests', 'bang', 'disorder', 'size', 'floor', 'option', 'numbers', 'hb', 'subconscious', 'testing', 'stranger', 'question', 'delete', 'strangers', 'suggest', 'minimal']\n",
      "\n",
      "Biased words towards target set 2\n",
      "['sense', 'gets', 'books', 'elon', 'pill', 'became', 'fell', 'died', 'beaten', 'continues', 'proud', 'shot', 'wish', 'killed', 'kept', 'went', 'poor', 'bought', 'worked', 'felt', 'thanks', 'rock', 'hell', 'saved', 'sir', 'despise', 'en', 'fixed', 'petty', 'turned', 'incel', 'southern', 'voice', 'father', 'ripped', 'videos', 'decades', 'teen', 'pushed', 'bravo', 'sacrifice', 'piller', 'kudos', 'pillers', 'lone', 'rollo', 'theories', 'enlightened', 'neglect', 'gibson', 'jp', 'pilled', 'daring']\n"
     ]
    }
   ],
   "source": [
    "print('Biased words towards target set 1')\n",
    "print( [w for w in b1.keys()] )\n",
    "print()\n",
    "print('Biased words towards target set 2')\n",
    "print( [w for w in b2.keys()] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qvav-HBtrQlp"
   },
   "source": [
    "## Clustering similar words (K-means + silhouette)\n",
    "\n",
    "Here,  we group our language biases in more general clusters. We do so using the K-means clustering algorithm, and use silhouette scores to validate the consistency within our clusters of data. \n",
    "\n",
    "In general, this results in words with similar meanings being clustered together. Clustering allows the biased words to be better interpretable, as their context becomes clearer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "executionInfo": {
     "elapsed": 153194,
     "status": "ok",
     "timestamp": 1596264125376,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "ZbG2pMTgrQlq",
    "outputId": "66db6d0e-8b83-4e3c-8954-832b0a7ddb1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Testing range(2, 27) clusters]\n",
      "[Silhouette values [0.02778791379787158, 0.039494154611549294, 0.03097213238586305, 0.02849508454245288, 0.029138486813817616, 0.03364082388010908, 0.03580269135571707, 0.029613497051388105, 0.031309845113033835, 0.02865506303348313, 0.026623522602726622, 0.04035151394937638, 0.04070602246764658, 0.020925117984820867, 0.028689536328708305, 0.030793969492096136, 0.039709379650529876, 0.03110092289034547, 0.036038241225620306, 0.03953761265633443, 0.01985625931656367, 0.04057242812495361, 0.026288204512068782, 0.03941390983501236, 0.03390974433541971]\n",
      "[Max silhouette,  0.04070602246764658 ; index_k:  12 ]\n",
      "[Testing range(2, 25) clusters]\n",
      "[Silhouette values [0.11422078253116053, 0.0416177725243335, 0.08181303691625372, 0.08819785498413106, 0.06071873752250599, 0.042835978059658664, 0.03871094179160861, 0.04952982853902574, 0.04927045828375377, 0.0479741664350335, 0.047474143915192, 0.04634766724731882, 0.009379786831844074, 0.022634556122341123, -0.00019171249871389597, 0.009587301189827546, 0.02278834171130132, 0.014874119041567014, 0.008788930895810664, 0.031549640952464764, 0.0370076666639194, 0.018305898427862843, 0.031198590637289473]\n",
      "[Max silhouette,  0.11422078253116053 ; index_k:  0 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn\n",
    "\n",
    "'''\n",
    "TARGET SET 1\n",
    "'''\n",
    "t1_embeddings = [b1[w]['wv'] for w in b1] # t1 embeddings = list of embeddings of words biased towards target set 1\n",
    "t1_words = [w for w in b1.keys()]\n",
    "\n",
    "# Clustering\n",
    "rangek = range(2, int((len(t1_embeddings)/2)-1) ) # Clusters should be min size 2 at max half of the amount of words (speeding up + forcing clusters)\n",
    "print('[Testing', rangek, 'clusters]')\n",
    "kmeans_p = [ KMeans(n_clusters=k).fit_predict(t1_embeddings) for k in rangek] \n",
    "kmeans_sil = [ sklearn.metrics.silhouette_score(t1_embeddings, labels) for labels in kmeans_p] \n",
    "print('[Silhouette values', kmeans_sil)\n",
    "indexmaxsil =  kmeans_sil.index(max(kmeans_sil))\n",
    "print('[Max silhouette, ', max(kmeans_sil), '; index_k: ',indexmaxsil,']')\n",
    "\n",
    "# Aggregating all clusters from same index in list\n",
    "clusters1 = {}\n",
    "for i, index in enumerate(kmeans_p[indexmaxsil]): # returns list of cluster index, telling you which cluster each word belongs to \n",
    "    if(index in clusters1):\n",
    "        clusters1[index].append(t1_words[i])\n",
    "    else:\n",
    "        clusters1[index]  = [t1_words[i]]\n",
    "        \n",
    "        \n",
    "'''\n",
    "TARGET SET 2\n",
    "'''\n",
    "t2_embeddings = [b2[w]['wv'] for w in b2]\n",
    "t2_words = [w for w in b2.keys()]\n",
    "\n",
    "# Clustering\n",
    "rangek = range(2, int((len(t2_embeddings)/2)-1) )\n",
    "print('[Testing', rangek, 'clusters]')\n",
    "kmeans_p = [ KMeans(n_clusters=k).fit_predict(t2_embeddings) for k in rangek ] \n",
    "kmeans_sil = [ sklearn.metrics.silhouette_score(t2_embeddings, labels) for labels in kmeans_p] \n",
    "print('[Silhouette values', kmeans_sil)\n",
    "indexmaxsil =  kmeans_sil.index(max(kmeans_sil))\n",
    "print('[Max silhouette, ', max(kmeans_sil), '; index_k: ',indexmaxsil,']')\n",
    "\n",
    "clusters2 = {}\n",
    "for i, index in enumerate(kmeans_p[indexmaxsil]):\n",
    "    if(index in clusters2):\n",
    "        clusters2[index].append(t2_words[i])\n",
    "    else:\n",
    "        clusters2[index]  = [t2_words[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1520,
     "status": "ok",
     "timestamp": 1596267292821,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "FKIP7T9u0prK",
    "outputId": "e32fe22e-7085-4d5e-8a0e-254e091f0791"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([['fact', 'reality', 'conversation', 'moment', 'forget', 'question'], ['brain', 'chances', 'size', 'minimal'], ['food'], ['body', 'obvious', 'pussy', 'rationalization', 'partner', 'assume', 'answer', 'exclusivity', 'ignoring', 'levels', 'okay', 'increasing', 'physical', 'disorder', 'option', 'subconscious', 'suggest'], ['apps', 'problems', 'issue'], ['text', 'phone', 'delete'], ['spinning', 'hamster', 'chick', 'mine', 'chicks', 'hubby', 'plate', 'hb', 'stranger', 'strangers'], ['anyone', 'everyone', 'everybody'], ['lawyer'], ['number', 'numbers'], ['hang', 'bang'], ['clubs'], ['tests', 'testing'], ['floor']])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = clusters1.values()\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5lZ4_1WeYr9"
   },
   "source": [
    "Let's print the clusters we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 150563,
     "status": "ok",
     "timestamp": 1596264125378,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "CIXYxKM9eB5z",
    "outputId": "236466e0-a40f-48d1-c8a7-54c3ee553454",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters target set 1\n",
      "[['fact', 'reality', 'conversation', 'moment', 'forget', 'question'], ['brain', 'chances', 'size', 'minimal'], ['food'], ['body', 'obvious', 'pussy', 'rationalization', 'partner', 'assume', 'answer', 'exclusivity', 'ignoring', 'levels', 'okay', 'increasing', 'physical', 'disorder', 'option', 'subconscious', 'suggest'], ['apps', 'problems', 'issue'], ['text', 'phone', 'delete'], ['spinning', 'hamster', 'chick', 'mine', 'chicks', 'hubby', 'plate', 'hb', 'stranger', 'strangers'], ['anyone', 'everyone', 'everybody'], ['lawyer'], ['number', 'numbers'], ['hang', 'bang'], ['clubs'], ['tests', 'testing'], ['floor']]\n",
      "Clusters target set 2\n",
      "[['sense', 'gets', 'books', 'elon', 'became', 'fell', 'died', 'beaten', 'continues', 'proud', 'shot', 'wish', 'killed', 'kept', 'went', 'poor', 'bought', 'worked', 'felt', 'thanks', 'rock', 'hell', 'saved', 'sir', 'despise', 'en', 'fixed', 'petty', 'turned', 'incel', 'southern', 'voice', 'father', 'ripped', 'videos', 'decades', 'teen', 'pushed', 'bravo', 'sacrifice', 'kudos', 'lone', 'rollo', 'theories', 'enlightened', 'neglect', 'gibson', 'jp', 'daring'], ['pill', 'piller', 'pillers', 'pilled']]\n"
     ]
    }
   ],
   "source": [
    "print('Clusters target set 1')\n",
    "print( list( clusters1.values()) )        \n",
    "\n",
    "print('Clusters target set 2')\n",
    "print( list( clusters2.values()) )        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4d296dd9a571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclusters1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mlabels1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclusters1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0membs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clusters1' is not defined"
     ]
    }
   ],
   "source": [
    "# prepare the datat for the tsne in order to create the plot\n",
    "\n",
    "labels1 = [] # labels = words\n",
    "y1 = []\n",
    "embs1 = []\n",
    "for k in clusters1:\n",
    "  labels1 += clusters1[k]\n",
    "embs1 = [model[j] for j in labels1]\n",
    "y1 = len(labels1)*[0]\n",
    "\n",
    "print(labels1)\n",
    "print(len(labels1))\n",
    "print(len(embs1))\n",
    "print(y1)\n",
    "\n",
    "labels2 = []\n",
    "y2 = []\n",
    "embs2 = []\n",
    "for k in clusters2:\n",
    "    labels2 += clusters2[k]\n",
    "embs2 = [model[j] for j in labels2]\n",
    "y2 = len(labels2)*[1]\n",
    "\n",
    "print(labels2)\n",
    "print(len(labels2))\n",
    "print(len(embs2))\n",
    "print(y2)\n",
    "\n",
    "labels = labels1 +labels2\n",
    "y      = y1 +y2\n",
    "embs   = embs1 + embs2\n",
    "print(labels)\n",
    "print(len(labels))\n",
    "print(len(embs))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "p4XnvP_1Wml4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-7ee9eddd08dd>:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  X = model[model.wv.vocab]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-7ee9eddd08dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_tsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m         \"\"\"\n\u001b[0;32m--> 891\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0mdegrees_of_freedom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m         return self._tsne(P, degrees_of_freedom, n_samples,\n\u001b[0m\u001b[1;32m    801\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0mopt_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0mopt_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_iter_without_progress'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_without_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             params, kl_divergence, it = _gradient_descent(obj_func, params,\n\u001b[0m\u001b[1;32m    857\u001b[0m                                                           **opt_args)\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compute_error'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_convergence\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     error = _barnes_hut_tsne.gradient(val_P, X_embedded, neighbors, indptr,\n\u001b[0m\u001b[1;32m    260\u001b[0m                                       \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                                       \u001b[0mdof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# this works,but the tsne should be estimated for all words in the model otherwise the tsne dimensions wont be correct\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# if words apepar very close together, change random state to obtain better visual representations\n",
    "# dimensional reduction of T-SNE can be done through different ways; random_state determines which is taken\n",
    "tsne = TSNE(perplexity=40, n_components=2, init='pca', n_iter=5000, random_state=42)\n",
    "X_tsne = tsne.fit_transform(embs) # X_tsne = [[1,2], [3,4], [5,6]]\n",
    "\n",
    "\n",
    "# split the X and Y coordinates\n",
    "embsx = [x[0] for x in X_tsne]\n",
    "embsy = [x[1] for x in X_tsne]\n",
    "color = ['blue' if i == 0 else 'red' for i in y]\n",
    "\n",
    "plt.figure(figsize=(16, 16)) \n",
    "for i in range(len(embs)):\n",
    "    plt.scatter(embsx[i],embsy[i], c=color[i])\n",
    "    plt.annotate( labels[i],\n",
    "                  xy=(embsx[i],embsy[i]),\n",
    "                  xytext=(6, 3),\n",
    "                  textcoords='offset points',\n",
    "                  ha='right',\n",
    "                  va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AW0lJqLpqayn"
   },
   "source": [
    "\n",
    "## Creating your own target sets\n",
    "\n",
    "If you want, you can try to expose the biases of your own dataset. You can use the target sets defined below, but also create your own. For instance, if you'd want to see which words are biased towards the political left and right, you could create two target sets \"Left\" and \"Right\" with respective attributes such as `left-wing, leftist, progressive`, and `right-wing, reactionary, conservative`. The more expansive and accurate you can make your target set, the better the system will work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riINX9wVyFRJ"
   },
   "source": [
    "## Existing target sets - details\n",
    "\n",
    "*Gender target sets taken from Nosek, Banaji, and Greenwald 2002.*\n",
    "\n",
    "Female: `sister, female, woman, girl, daughter, she, hers, her`.\n",
    "\n",
    "Male: `brother, male, man, boy, son, he, his, him`.\n",
    "\n",
    "\n",
    "*Religion target sets taken from Garg et al. 2018.*\n",
    "\n",
    "Islam: `allah, ramadan, turban, emir, salaam, sunni, koran, imam, sultan, prophet, veil, ayatollah, shiite, mosque, islam, sheik, muslim, muhammad`.\n",
    "\n",
    "Christianity: `baptism, messiah, catholicism, resurrection, christianity, salva-tion, protestant, gospel, trinity, jesus, christ, christian, cross,catholic, church`.\n",
    "\n",
    "*Racial target sets taken from Garg et al. 2017*\n",
    "\n",
    "White last names: `harris, nelson, robinson, thompson, moore, wright, anderson, clark, jackson, taylor, scott, davis, allen, adams, lewis, williams, jones, wilson, martin, johnson`.\n",
    "\n",
    "Hispanic last names: `ruiz, alvarez, vargas, castillo, gomez, soto,gonzalez, sanchez, rivera, mendoza, martinez, torres, ro-driguez, perez, lopez, medina, diaz, garcia, castro, cruz`.\n",
    "\n",
    "Asian last names: `cho, wong, tang, huang, chu, chung, ng,wu, liu, chen, lin, yang, kim, chang, shah, wang, li, khan,singh, hong`.\n",
    "\n",
    "Russian last names: `gurin, minsky, sokolov, markov, maslow, novikoff, mishkin, smirnov, orloff, ivanov, sokoloff, davidoff, savin, romanoff, babinski, sorokin, levin, pavlov, rodin, agin`.\n",
    "\n",
    "\n",
    "*Career/family target sets taken from Garg et al. 2018.*\n",
    "\n",
    "Career: `executive, management, professional, corporation, salary, office, business, career`.\n",
    "\n",
    "Family: `home, parents, children, family, cousins, marriage, wedding, relatives.Math: math, algebra, geometry, calculus, equations, computation, numbers, addition`.\n",
    "\n",
    "\n",
    "*Arts/Science target sets taken from Garg et al. 2018.*\n",
    "\n",
    "Arts: `poetry, art, sculpture, dance, literature, novel, symphony, drama`.\n",
    "\n",
    "Science: `science, technology, physics, chemistry, Einstein, NASA, experiment, astronomy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHCHMAPYTCVX"
   },
   "source": [
    "### Sources\n",
    "\n",
    "Nosek, B. A., Banaji, M. R., & Greenwald, A. G. (2002). Harvesting implicit group attitudes and beliefs from a demonstration web site. Group Dynamics, 6(1), 101–115. https://doi.org/10.1037/1089-2699.6.1.101\n",
    "\n",
    "Garg, N., Schiebinger, L., Jurafsky, D., & Zou, J. (2017). Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes, 1–33."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DADD Language Biases Workshop.ipynb",
   "provenance": [
    {
     "file_id": "1IgWcAC_gXRgwysSbqDr5ZIO9bxSVb6dl",
     "timestamp": 1601554822392
    },
    {
     "file_id": "1HXx9kevQetMI9pE0B9XCTFjgUHiGC4hm",
     "timestamp": 1594637500809
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
