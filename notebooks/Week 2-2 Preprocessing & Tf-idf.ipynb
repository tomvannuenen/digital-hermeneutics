{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bhs6S4deZFWW"
   },
   "source": [
    "<p style=\"float: left;\">\n",
    "    <span style=\"font-weight: bold; line-height: 24px; font-size: 16px;\">\n",
    "        DIGHUM160 - Critical Digital Humanities\n",
    "        <br />\n",
    "        Digital Hermeneutics \n",
    "    </span>\n",
    "    <br >\n",
    "    <span style=\"line-height: 22x; font-size: 14x; margin-top: 10px;\">\n",
    "        Week 2-2: Preprocessing and tf-idf <br />\n",
    "        Created by Tom van Nuenen (tom.van_nuenen@kcl.ac.uk)\n",
    "    </span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ekf1a33ZFWY"
   },
   "source": [
    "# Preprocessing and comparing subreddits\n",
    "\n",
    "In today's notebook we will (1) learn how to preprocess text from a reddit DataFrame, and (2) learn to implement tf-idf. It's a lot to get through!\n",
    "\n",
    "Tf-idf allows us to compare different related subreddits, in order to find the most distinctive words in a particular subreddit. It can also help us to find similar posts to ones we're interested in.\n",
    "\n",
    "Please read the instructions and fill in the coding assignments throughout the notebook. \n",
    "\n",
    "**After completing this notebook, you will be able to:**\n",
    "- Preprocess Reddit data, including removing punctuation, tokenizing, and lemmatizing;\n",
    "- Understand how tf-idf can be used to compare datasets;\n",
    "- Find most-distinctive words in a subreddit using tf-idf;\n",
    "- Find similar posts using tf-idf.\n",
    "\n",
    "There are several basic programming exercises scattered throughout for those who need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXhTdS5Ph2ND"
   },
   "source": [
    "## Import packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-uQdxtmCZFWY"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import re \n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7RfwrCNiGvn"
   },
   "source": [
    "Let's get our files from Google Drive. These three datasets are taken from subreddits that are, to different degrees, related to the \"Manosphere\" and the men's right movement: r/theredpill, r/seduction and r/mgtow (Men Going Their Own Way). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjxJwncSo4_M"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81Pk2xVGoMoe"
   },
   "outputs": [],
   "source": [
    "downloaded = drive.CreateFile({'id':\"1hN5eqCYVZOX_O0i8waJUQxDlJjqDMenK\"})   # replace the id with id of file you want to access\n",
    "downloaded.GetContentFile('TRP-submissions.csv')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Awlk7rs7ZFW4"
   },
   "outputs": [],
   "source": [
    "# r/theredpill\n",
    "trp = pd.read_csv(\"TRP-submissions.csv\", lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYUfVU8xp3eN"
   },
   "outputs": [],
   "source": [
    "downloaded = drive.CreateFile({'id':\"1fOe3l9dLKb51jrwqUNOvwO4A7F7sM6Xx\"})   # replace the id with id of file you want to access\n",
    "downloaded.GetContentFile('seduction-submissions.csv')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFKIEs96ZFW7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# r/seduction\n",
    "sed = pd.read_csv(\"seduction-submissions.csv\", lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "53MXtacKp5nA"
   },
   "outputs": [],
   "source": [
    "downloaded = drive.CreateFile({'id':\"15uxSxMupQ3zk4nevOgxkV2sBvDrSawnr\"})   # replace the id with id of file you want to access\n",
    "downloaded.GetContentFile('mgtow-submissions.csv')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nc7OYxIvZFW-"
   },
   "outputs": [],
   "source": [
    "# r/mgtow\n",
    "mgtow = pd.read_csv(\"mgtow-submissions.csv\", lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b5LusX7JXckS"
   },
   "source": [
    "How big are our datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J433TZm4XcEa"
   },
   "outputs": [],
   "source": [
    "print(\"r/seduction: \" + str(len(sed)))\n",
    "print(\"r/theredpill: \" + str(len(trp)))\n",
    "print(\"r/mgtow: \" + str(len(mgtow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_dVKTaN8LutK"
   },
   "source": [
    "### Removing rows\n",
    "Missing values (`NaN`) in a DataFrame can cause a lot of errors. In general, it's a god idea to get rid of those rows whose \"selftext\" is missing. Here's an example of how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6khNIe0kLvOf"
   },
   "outputs": [],
   "source": [
    "data = {'Name':['Sai', 'Jack', 'Angela', 'Matt', 'Alisha', 'Ricky'],'Age':[28,34,None,42, \"[removed]\", \"[deleted]\"]}\n",
    "df = pd.DataFrame(data) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0gj5FBXM8NG"
   },
   "outputs": [],
   "source": [
    "clean_df = df.dropna(subset=['Age'])  # Drop NaN in the column 'Age'\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTWDsJFgYRle"
   },
   "source": [
    "We can also remove cells with particular text in it (this is relevant as Reddit datasets often contain posts that are removed or deleted!). We do this by using the `.isin()` method. We remove this selection from our DataFrame by using `~`. See if you understand how this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IMqUgnNdYBHW"
   },
   "outputs": [],
   "source": [
    "cleaner_df = clean_df[~clean_df['Age'].isin(['[removed]', '[deleted]' ])]\n",
    "cleaner_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EaNTzUNcNqmz"
   },
   "source": [
    "Your turn! Drop the missing (`NaN`), removed (`[removed]`) and deleted (`[deleted]`) values from our three DataFrames and assign the result to the same variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Othbm6juMVmC"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HK8aZS3SZyZB"
   },
   "source": [
    "Let's see if that shrinks our DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "duFmEc9OZx00"
   },
   "outputs": [],
   "source": [
    "print(\"r/seduction: \" + str(len(sed)))\n",
    "print(\"r/theredpill: \" + str(len(trp)))\n",
    "print(\"r/mgtow: \" + str(len(mgtow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1wWfN7XZ1oE"
   },
   "source": [
    "Looks like it did!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xlu74LCjjNhm"
   },
   "source": [
    "### Getting a slice\n",
    "It's usually good to start small and see if all of your preprocessing functions work as expected, then scale up. Let's start with a slice of 100 posts based on the highest score. Yesterday, we saw how we can do that using the `.sort_values()` method. Recall that sorting and slicing a dataframe works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcpxg8ezjNxA"
   },
   "outputs": [],
   "source": [
    "# Sorting\n",
    "sorted_df = trp.sort_values(by=['score'], ascending=False)\n",
    "\n",
    "# Slicing\n",
    "sliced_df = trp[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ON6sYHXbiDac"
   },
   "source": [
    "Your turn: combine these two expressions to filter the 10 highest-scoring posts of `trp`. Assign it to a new variable: `trp_10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elYKcpP0ZFXB"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pn6giFWubji1"
   },
   "source": [
    "## Preprocessing data\n",
    "\n",
    "Great, we got our data. Now, we need to preprocess it. This includes:\n",
    "1. Removing special characters and punctuation\n",
    "2. Tokenizing\n",
    "3. Removing stopwords\n",
    "4. Part of Speech (POS) tagging & filtering\n",
    "5. Stemming / lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "22GAJ_PQnjT-"
   },
   "source": [
    "### Programming basics: Functions\n",
    "\n",
    "Do these exercises if you need to learn about functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3_9zO6jDniYG"
   },
   "source": [
    "A function is like a little program. It's basically a block of code which only runs when it is called. It looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fWIIUx3bnwx1"
   },
   "outputs": [],
   "source": [
    "def my_function():\n",
    "  print(\"Hello, world!\")\n",
    "\n",
    "my_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hQkjUKcJoQxj"
   },
   "source": [
    "Functions can take some data as input, called \"parameters\". It can transform that input, and can `return` you some output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m1FKBPaZoZXN"
   },
   "outputs": [],
   "source": [
    "def my_function(inp):\n",
    "  return \"Hello, \" + inp\n",
    "\n",
    "my_function(\"you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hff39BoGo0Rt"
   },
   "source": [
    "Create a function called `multiplier` that takes some number as a parameter, multiplies that number by 10, and `return`s it. Then run the cell to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3l3hcsLsooW9"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rEaV-cyYbjjm"
   },
   "source": [
    "### Removing punctuation\n",
    "\n",
    "First, have a look at how to use `string.punctuation` to get rid of some punctuation characters. `string.punctuation` is not a function: it's a pre-initialized string which we can use to get rid of punctuation in a string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EW9WvfCTmMQ2"
   },
   "outputs": [],
   "source": [
    "old_sent = \"I. don't. know. why. I'm. speaking. like. this.\"\n",
    "new_sent = \"\"\n",
    "for ch in old_sent:\n",
    "  if ch not in string.punctuation:\n",
    "    new_sent += ch\n",
    "\n",
    "new_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cXwW6hxsmxYk"
   },
   "source": [
    "Your turn! Try to create a function called `strip_punctuation` that strips punctuation from a string. It takes a string as a parameter, and returns a new string with all punctuation stripped out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3NPw95Objjn"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8Bme_0wjuuF"
   },
   "source": [
    "In the following, we're running our function. Try to follow along!\n",
    "\n",
    "1. We create an empty list called `trp_strip_punct`;\n",
    "2. We run a `for`-loop that iterates over all the \"selftexts\" in the `trp_10` DataFrame, and that applies our function to each text; \n",
    "3. We save the result in a new variable;\n",
    "4. We print our new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VGVq8rYRjwZp"
   },
   "outputs": [],
   "source": [
    "trp_strip_punct = []\n",
    "for t in trp_10.selftext:\n",
    "  trp_strip_punct.append(strip_punctuation(t))\n",
    "\n",
    "trp_strip_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sAGyS-eEqQTk"
   },
   "source": [
    "### Tokenizing\n",
    "Next, we need to create a tokenizer. Create another list called `trp_tokens`, then use another for-loop that applies NLTK's `word_tokenize()` method on each entry of our `trp_strip_punct` list. Use `.append()` instead of `.extend()` so that your loop creates 100 lists of tokens (instead of one long list of tokens, like we did yesterday). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXwV2ofxbji2"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kcutO8_8ltVp"
   },
   "source": [
    "`trp_tokens` is a list of lists: each list contains the individual tokens of a post. What if we want to access a list within a list? It works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nfkKqBPels49"
   },
   "outputs": [],
   "source": [
    "list1 = [[10,13,17],[3,5,1],[13,11,12]]\n",
    "list1[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5NYhlQlalJih"
   },
   "source": [
    "Your turn! Print out the first 10 entries in the first entry of the `trp_tokens` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W2FDyZXNbji7"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rNmrlsxsGbB"
   },
   "source": [
    "### Programming basics: Sets\n",
    "Do these exercises if you need to learn about sets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ToZw1ALtnnXc"
   },
   "source": [
    "A set is an **unordered** and **unindexed** collection. This makes them a different data type from lists, which are ordered, and from dictionaries, which are indexed. You can use sets to rapidly iterate through a list, when the order within that list doesn't matter. \n",
    "\n",
    "In Python sets are written with curly brackets, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3LnqWzBescvZ"
   },
   "outputs": [],
   "source": [
    "my_set = {\"apple\", \"pear\", \"orange\"}\n",
    "print(my_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXt8whadslGm"
   },
   "source": [
    "Note that the order is not preserved!\n",
    "\n",
    "You can also create a set from a different data type. What do you think will happen in the below expression? Think about it for a second, then run it to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3WsxeZut9IU"
   },
   "outputs": [],
   "source": [
    "set(\"I am going to work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kk9x-vbuKbd"
   },
   "source": [
    "Your turn! First, assign a new variable to a list of some random numbers. Then, force that list into a set, and print it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gd7Zr-N-tpLH"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfOI3Zm2bjj1"
   },
   "source": [
    "### Removing stopwords\n",
    "Next, let's remove stopwords. We can do so using NLTK's stopwords list, which we imported above. Let's have a look at some of these stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15WB6xkKqquL"
   },
   "outputs": [],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YBP6qhcrAvf"
   },
   "source": [
    "Iterating through this list for *every* word in our two corpora is going to take a long time, so let's turn it into a set. This saves us some time, as sets are less memory-intensive.\n",
    "\n",
    "Remember, when creating a set it shouldn't matter which order items are in – and for our stopwords list, that is the case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JXYe77ZjtDtX"
   },
   "outputs": [],
   "source": [
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PkkZ6pV3_e6Y"
   },
   "source": [
    "Your turn! \n",
    "1. Create a function called `strip_stopwords()` that takes `tokens` as a parameter;\n",
    "2. In the function, create a list named `no_stop`; \n",
    "3. Turn `stopwords.words('english')` into a set (like above), then assign it to a variable named `stop`;\n",
    "4. Run a for-loop that fills the `no_stop` list with only those tokens that are *not* in `stop` (you need an `if`-statement!);\n",
    "5. Finally, `return` the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO_6RsDkbjj1"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCf2ACawAVTe"
   },
   "source": [
    "Run the following line of code to see if it worked. You should get a printout of the first 10 tokens in the first post of `trp` – without the stopwords of course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vxa_E-XEbjj-"
   },
   "outputs": [],
   "source": [
    "# Run this\n",
    "trp_clean = [strip_stopwords(tokens) for tokens in trp_tokens]\n",
    "trp_clean[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S3czfumObjjF"
   },
   "source": [
    "### Stemming\n",
    "Tokenizers are great, but they're often not perfect. Look at the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EmNSZkGLbjjH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_tokenize(\"Why won't this work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ml1MyMB-bjjM"
   },
   "source": [
    "Looks like it did a pretty good job, except it considers \"wo\" and \"n't\" as different words.. Annoying. This is where **stemming** and **lemmatizing** come in handy. These are two text normalization techniques that are used to prepare text, words, and documents for further processing. \n",
    "\n",
    "See [this link](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python?utm_source=adwords_ppc&utm_campaignid=1455363063&utm_adgroupid=65083631748&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=332602034361&utm_targetid=dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=1012831&gclid=Cj0KCQjwgJv4BRCrARIsAB17JI4kMKOUrJcdearlvPx4kl3VNVcqeZz-oeTSlbgikK3tJbXMrAmWTCwaAvUzEALw_wcB) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9ZVhgbdbjjN"
   },
   "source": [
    "**Stemming** is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language. First, let's load our stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AxFw-sobjjN"
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Xt4sEh0bjjR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for each in [\"think\", \"thinker\", \"thinking\"]:\n",
    "    print(stemmer.stem(each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ltayyO3SbjjU"
   },
   "source": [
    "...but stemming doesn't always produce the prettiest results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5muPaUpubjjV"
   },
   "outputs": [],
   "source": [
    "for each in [\"create\", \"creating\", \"creator\"]:\n",
    "    print(stemmer.stem(each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJ5EYLZnbjja"
   },
   "source": [
    "### Lemmatizing\n",
    "A lemma is the canonical, dictionary or citation form of a word. For instance, the lemma for \"thinks\" is \"think.\" Lemmatization, in other words, is the process of converting a word to its base form.\n",
    "\n",
    "Lemmatizing your data typically is a bit less intrusive than stemming it. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7DHGwVwDbjjb"
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fx4Zwmjgbjjf"
   },
   "outputs": [],
   "source": [
    "for each in [\"trade\", \"trades\", \"trading\", \"trader\", \"traders\"]:\n",
    "    print(lemmatizer.lemmatize(each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AyQ4CdYrwfg8"
   },
   "source": [
    "Your turn! \n",
    "1. Create a function called `lemmatize()` that takes `tokens` as a parameter;\n",
    "2. Create a new list called `lemmas`\n",
    "3. In the function, assign `nltk.stem.WordNetLemmatizer()` to a variable called `lemmatizer`, like above; \n",
    "4. Run a `for`-loop that uses `lemmatizer.lemmatize(each)` to lemmatize each token in `tokens`; append the output to our `lemmas` list;  \n",
    "5. Finally, `return` the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Zk1vuxMwfg9"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XW3YzyYGx_3v"
   },
   "source": [
    "Run the following line of code to see if it worked. You should see that the token 'masters' has been changed to 'master'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZ32Gkk7x_3v"
   },
   "outputs": [],
   "source": [
    "# Run this\n",
    "trp_lemmas = [lemmatize(tokens) for tokens in trp_clean]\n",
    "trp_lemmas[0][30:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wqgYPvK2rL7"
   },
   "source": [
    "### Forcing to string\n",
    "Sometimes, when we have a list, we actually want a string. For instance, some libraries of NLP tools require strings as input. In those cases, we can force lists into strings by applying the list `.join` method. Let's use it to turn the first entry of our `trp_lemmas` list into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "be-Xgn3n2qlJ"
   },
   "outputs": [],
   "source": [
    "trp_str = ' '.join(trp_lemmas[0])\n",
    "trp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4fBg61yGZFXF"
   },
   "source": [
    "## Putting it all together\n",
    "After all that, you should be well-equipped to understand this preprocessing function. It takes a DataFrame in, removes the empty values, then removes punctuation, tokenizes and lemmatizes the selftext. It then spits the text back out as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K6Pr-0LEZFXG"
   },
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    \"\"\"POS tags and filters DF by nouns\"\"\"\n",
    "    dfLength = len(df)\n",
    "    total = \"\"\n",
    "    counter = 0\n",
    "    clean = df[~df['selftext'].isin(['[removed]', '[deleted]' ])].dropna(subset=['selftext'])\n",
    "    for text in clean['selftext']:\n",
    "        # turn to lowercase\n",
    "        text = text.lower()\n",
    "        # remove punctuation\n",
    "        text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
    "        # tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        # lemmatize\n",
    "        lemmas = ' '.join([wordnet_lemmatizer.lemmatize(token) for token in tokens])\n",
    "        # save\n",
    "        total += lemmas\n",
    "        counter += 1\n",
    "        if counter % 100 == 0:\n",
    "            print(\"Saved \" + str(counter) + \" out of \" + str(dfLength) + \" entries\") \n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZonK-EC4kcJ"
   },
   "source": [
    "Let's run our function on the first 1000 entries of our DataFrames (just to save some time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0ibFc_DZFXI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trp_pp = preprocessing(trp[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qbdNoIMNZFXK"
   },
   "outputs": [],
   "source": [
    "sed_pp = preprocessing(sed[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hr86_R4xZFXP"
   },
   "outputs": [],
   "source": [
    "mgtow_pp = preprocessing(mgtow[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EpWynSoy4t8o"
   },
   "source": [
    "## Tf-idf\n",
    "Tf–idf or TFIDF, short for *term frequency–inverse document frequenc*y, is a numerical statistic that reflects how important a word is to a document in a collection or corpus.\n",
    "Tf_idf is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document (the term frequency, or tf), and is offset by the number of documents in the corpus that contain the word (the inverse document frequency, or idf). This helps to adjust for the fact that some words appear more frequently in general – such as articles and prepositions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yvxZ-5f6ZFWe"
   },
   "source": [
    "### Testing tf-idf with a toy dataset\n",
    "\n",
    "Let's try tf-idf out with a toy dataset. Here we have three documents about Python, but with different meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjjWn-phZFWf"
   },
   "outputs": [],
   "source": [
    "document1 = \"\"\"Python is a 2000 made-for-TV horror movie directed by Richard\n",
    "Clabaugh. The film features several cult favorite actors, including William\n",
    "Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy,\n",
    "Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the\n",
    "A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean\n",
    "Whalen.\"\"\"\n",
    "\n",
    "document2 = \"\"\"Python, from the Greek word (πύθων/πύθωνας), is a genus of\n",
    "nonvenomous pythons[2] found in Africa and Asia. Currently, 7 species are\n",
    "recognised.[2] A member of this genus, P. reticulatus, is among the longest\n",
    "snakes known.\"\"\"\n",
    "\n",
    "document3 = \"\"\"Monty Python (also collectively known as the Pythons) are a British \n",
    "surreal comedy group who created the sketch comedy television show Monty Python's \n",
    "Flying Circus, which first aired on the BBC in 1969. Forty-five episodes were made \n",
    "over four series.\"\"\"\n",
    "\n",
    "document4 = \"\"\"Python is an interpreted, high-level, general-purpose programming language. \n",
    "Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes \n",
    "code readability with its notable use of significant whitespace. Its language constructs and \n",
    "object-oriented approach aim to help programmers write clear, logical code for small and \n",
    "large-scale projects.\"\"\"\n",
    "\n",
    "document5 = \"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
    "manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
    "It is sometimes referred to as a \"Combat Magnum\".[1] It was first introduced\n",
    "in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
    "Colt Python targeted the premium revolver market segment.\"\"\"\n",
    "\n",
    "document6 = \"\"\"The Pythonidae, commonly known simply as pythons, from the Greek word python \n",
    "(πυθων), are a family of nonvenomous snakes found in Africa, Asia, and Australia. \n",
    "Among its members are some of the largest snakes in the world. Eight genera and 31\n",
    "species are currently recognized.\"\"\"\n",
    "\n",
    "test_list = [document1, document2, document3, document4, document5, document6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iVLHjD29ZFWi"
   },
   "source": [
    "We will be using Scikit-LEARN `TfidfVectorizer`. It is a class that basically allows us to create a matrix of word counts, and immediately transform them into tf-idf values. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) for the documentation if you want to learn more.\n",
    "\n",
    "In the second line below, we instantiate an object of the vectorizer. Then, we run it by applying the `fit_transform` method to our `test_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WeTAynM9xxxr"
   },
   "outputs": [],
   "source": [
    "# settings that you use for count vectorizer will go here\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, decode_error='ignore', stop_words='english',smooth_idf=True,use_idf=True)\n",
    "\n",
    "# fit and transform the texts\n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Uoj5xOi8vu_"
   },
   "source": [
    "Let's have a peek at our matrix by running the `.toarray()` method. \n",
    "This shows us one value per word in the total vocabulary.\n",
    "\n",
    "Notice that we're printing the vectors at index 2: this shows us the tf-idf features of the words in `document3` (due to zero-based indexing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRjP38dV-7I7"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer_vectors.toarray()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XGuoetn0ASpf"
   },
   "source": [
    "We can also have a look at the words in our total vocabulary by running `get_feature_names()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWPqOTR_AIM0"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1iydB28vAgEt"
   },
   "source": [
    "As you can see, the second word is '1969', which as you can see in the printout of our `.toarray()` is a distinctive word for Monty Python (the first airing of their TV show)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-XJjS79P6w2Y"
   },
   "source": [
    "### Putting distinctive words in a DataFrame\n",
    "We can now take out one vector (i.e., the tf-idf values of one text) that `.fit_transform()` yielded. We can put them in a DataFrame, and print out that DataFrame after sorting it based on the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fpzFSP6x9es"
   },
   "outputs": [],
   "source": [
    "# get the second vector out (for the second document)\n",
    "vector_tfidfvectorizer = tfidf_vectorizer_vectors[2] # Note that 2 refers to document3, due to zero-based indexing\n",
    "\n",
    "# place tf-idf values in a DataFrame\n",
    "df = pd.DataFrame(vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9qVYEm_ZFW2"
   },
   "source": [
    "Looks like it works! Through tf-idf, we have found the words that are most-distinctive of `document3`!\n",
    "\n",
    "Note that we have used `Tfidfvectorizer` here, which internally computes word counts, IDF values, and tf-idf scores for our dataset. If you only want to use the term frequency (term count) vectors for different tasks, you have to use `Tfidftransformer`. See e.g. [here](https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XviBUJMzbzg) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mq3QvXEcZFXS"
   },
   "source": [
    "## Using tf-idf on Reddit datasets\n",
    "Tf-idf is a basic but intuitive way to find words that are typical of a particular subreddit, when compared to other comparable subreddits.\n",
    "\n",
    "We'll implement scikit-learn's tf-idf functionality to find distinctive words for each document (i.e. subreddit). So we'll treat each subreddit we feed into the `TfidfTransformer` as a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aSQHlQliZFXV"
   },
   "outputs": [],
   "source": [
    "reddit_list = [trp_pp, sed_pp, mgtow_pp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mgKvuY96ZFXY"
   },
   "source": [
    "Your turn! Using `TfidfVectorizer`, repeat what we did with our test data, but this time with the `reddit_list`! It's exactly the same procedure – only the name of the list changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vRKbLpDZFXY"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DAxZynUCF8L"
   },
   "source": [
    "As you can see, there are still some HTML artifacts left, such as \"nbsp\". Next week, we'll look at how to remove these annoying tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_YoFv8pXZFXa"
   },
   "source": [
    "## Bonus: Using TF-IDF to find similar documents\n",
    "*Note: the below code is a bit more advanced, and for demonstration purposes. Don't worry if you don't fully get it!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qjvztHrBZFXb"
   },
   "source": [
    "We can also use tf-idf to work out the similarity between any pair of documents. So given one post or comment, we could see which posts or comments are most similar. This can be useful if you're trying to find other examples of a pattern you have found and want to explore further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aluiuMKvZFXb"
   },
   "source": [
    "This time, our \"documents\" will not be entire subreddits, but posts/submissions within one subreddit. Let's import the submissions and run the vectorizer without the preprocessing and lemmatizing. Tf-idf will still work this way, and this way, we will be able to read our posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04hxWFEXZFXc"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3), min_df = 0, stop_words = 'english')\n",
    "word_count_vectors = tfidf_vectorizer.fit_transform([post for post in trp['selftext']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "px4XE13U4Wwb"
   },
   "source": [
    "We'll start by finding a post with a clear topic. Let's grab the 11th entry in our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6n1IDR9y4Wwc"
   },
   "outputs": [],
   "source": [
    "trp['selftext'][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O6l6lqgF4B0L"
   },
   "source": [
    "This one seems to be about a lot of things, including 'alpha' and 'beta' men, narcissism, and reproduction. Let's have a quick look at the tfidf scores for the words in this submission to see if these words are indeed typical for this particular submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rpOde4Sr4AzJ"
   },
   "outputs": [],
   "source": [
    "# get a vector out\n",
    "vector_tfidfvectorizer = word_count_vectors[11] # change this number if you want to pick out a different vector / text\n",
    "\n",
    "# place tf-idf values in a pandas data frame\n",
    "df = pd.DataFrame(vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kiTzCg4xZFXi"
   },
   "source": [
    "Now let's find the top document(s). \n",
    "The fact that our documents are now in a vector space is convenient: it allows us to make use of mathematical similarity metrics.\n",
    "\n",
    "**Cosine similarity** is one metric used to measure how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space.\n",
    "\n",
    "Don't worry too much about this function for now: just run it and let's see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rbfy9VoCEiPc"
   },
   "outputs": [],
   "source": [
    "def find_similar(word_count_vectors, index, top_n = 5):   # you can change the `top_n` parameter if you want to retrieve more similar documents\n",
    "    cosine_similarities = linear_kernel(word_count_vectors[index:index+1], word_count_vectors).flatten()\n",
    "    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
    "    return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fuATLPrFZFXe"
   },
   "source": [
    "The above function finds similar words. It uses scikit-LEARN's `linear kernel`, which uses cosine similarity to find documents that are most alike.\n",
    "\n",
    "We can now throw the resulting scores and similar posts in a list, feed that list into a DataFrame, and check out one of them to see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6MuqmwKBZFXk"
   },
   "outputs": [],
   "source": [
    "cosine = []\n",
    "for index, score in find_similar(word_count_vectors, 11):\n",
    "  cosine.append(\n",
    "      {'cos_score': score, \n",
    "       'text': trp['selftext'][index]\n",
    "       }\n",
    "  )\n",
    "cosine_df = pd.DataFrame(cosine)\n",
    "cosine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dEbe-PoIizd"
   },
   "outputs": [],
   "source": [
    "cosine_df['text'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r16ecNp45gWB"
   },
   "source": [
    "This post does seem comparable! It's also about the difference between \"alpha\" and \"beta\" men, and sexual strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPcdL8H6ZFXm"
   },
   "source": [
    "## Reflection: hypothesis generation using tf-idf\n",
    "\n",
    "Think about a hypothesis or research question you could construct about your own subreddit based on these distinctive words and related posts."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pXhTdS5Ph2ND",
    "pn6giFWubji1",
    "22GAJ_PQnjT-",
    "6rNmrlsxsGbB",
    "4fBg61yGZFXF",
    "EpWynSoy4t8o",
    "Mq3QvXEcZFXS",
    "_YoFv8pXZFXa"
   ],
   "name": "Week 2-4 Preprocessing & tf-idf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
