{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Uds2RtAZOog"
   },
   "source": [
    "<div style=\"display: block; width: 100%; height: 120px;\">\n",
    "\n",
    "<p style=\"float: left;\">\n",
    "    <span style=\"font-weight: bold; line-height: 24px; font-size: 16px;\">\n",
    "        DIGHUM160 - Critical Digital Humanities\n",
    "        <br />\n",
    "        Digital Hermeneutics \n",
    "    </span>\n",
    "    <br >\n",
    "    <span style=\"line-height: 22x; font-size: 14x; margin-top: 10px;\">\n",
    "        Week 4-1: Word Embeddings<br />\n",
    "        Created by Tom van Nuenen (tom.van_nuenen@kcl.ac.uk)<br />\n",
    "    </span>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfjNP33MZOoi"
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Today, we'll have a look at word embeddings using Gensim's `word2vec` and `doc2vec` methods. \n",
    "\n",
    "The goal of word vector embedding models is to learn dense, numerical vector representations for each term in a corpus vocabulary. If successful, the vectors for each term encode information about the meaning or concept the term represents, as well as the relationship between it and other terms in the vocabulary. Word vector models are  fully unsupervised: they learn all of these meanings and relationships without any advance knowledge.\n",
    "\n",
    "After working through today's notebook, you'll be able to:\n",
    "\n",
    "1. Use Gensim's word2vec method to create word vectors for a corpus;\n",
    "2. Use these word vectors to reflect on implicit binaries and normativities in your data;\n",
    "3. Visualize topic models using K-means clustering.\n",
    "\n",
    "**Note: I encourage you to use your own dataset from here on out to start looking at semantic patterns, regularities, ideologies, myths, and so on, that are relevant to your essay.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "kRNNPkOdZOoj",
    "outputId": "3e88c103-7245-4705-aa99-a0cc5e8f0812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import smart_open\n",
    "import multiprocessing \n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Spacy\n",
    "import spacy \n",
    "\n",
    "# Plotting\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value\n",
    "\n",
    "# Clustering \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Suppressing warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBNH-bIFaNFI"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wDeidiecZOop"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "We'll start by cleaning up our data a bit. Let's load it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LaEjctJwtPnI"
   },
   "outputs": [],
   "source": [
    "downloaded = drive.CreateFile({'id':\"1nY9JtXoGJa7B-OmU6afh4qfcFGPCQIHW\"})   \n",
    "downloaded.GetContentFile('TRP-comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1cY6-yUcZOoq"
   },
   "outputs": [],
   "source": [
    "# load into df\n",
    "df_com = pd.read_csv(\"TRP-comments.csv\", lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yn3XOMNrZOo1"
   },
   "outputs": [],
   "source": [
    "# Get rid of empty values and reset index\n",
    "df_com = df_com[~df_com['body'].isin(['[removed]', '[deleted]' ])].dropna(subset=['body']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLs_XcxbDHzw"
   },
   "source": [
    "Let's create a small function that cleans up our text by removing all escape-tabs and escape-newlines, as well as all non symbol characters (except for the dot). It also normalizes spaces to a single character and removes leading and trailing spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEdJGFrnY_nG"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  # Normalize tabs and remove newlines\n",
    "  no_tabs = text.replace('\\t', ' ').replace('\\n', '');\n",
    "  # Remove all characters except A-Z and a dot.\n",
    "  alphas_only = re.sub(\"[^a-zA-Z\\.]\", \" \", no_tabs);\n",
    "  # Normalize spaces to 1\n",
    "  multi_spaces = re.sub(\" +\", \" \", alphas_only);\n",
    "  # Strip trailing and leading spaces\n",
    "  no_spaces = multi_spaces.strip();\n",
    "  return no_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_f5ZrCSglc2_",
    "outputId": "9f4606b7-1af8-48e8-e924-184a8e7dd2f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>idint</th>\n",
       "      <th>idstr</th>\n",
       "      <th>created</th>\n",
       "      <th>author</th>\n",
       "      <th>parent</th>\n",
       "      <th>submission</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>distinguish</th>\n",
       "      <th>textlen</th>\n",
       "      <th>body_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1650944</td>\n",
       "      <td>28219675724</td>\n",
       "      <td>t1_cyp9l58</td>\n",
       "      <td>1452171945</td>\n",
       "      <td>Stories_of_Red</td>\n",
       "      <td>t3_3zv11k</td>\n",
       "      <td>t3_3zv11k</td>\n",
       "      <td>If you marry this woman, do not ever blame her...</td>\n",
       "      <td>1897</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>191</td>\n",
       "      <td>If you marry this woman do not ever blame her ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2616308</td>\n",
       "      <td>30034466365</td>\n",
       "      <td>t1_dspqukd</td>\n",
       "      <td>1516029389</td>\n",
       "      <td>KirthWGersen</td>\n",
       "      <td>t3_7qk2y3</td>\n",
       "      <td>t3_7qk2y3</td>\n",
       "      <td>The scary thing about the Ansari case is that ...</td>\n",
       "      <td>1776</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1297</td>\n",
       "      <td>The scary thing about the Ansari case is that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2457822</td>\n",
       "      <td>29621801610</td>\n",
       "      <td>t1_dlw20ei</td>\n",
       "      <td>1503252966</td>\n",
       "      <td>Thotwrecker</td>\n",
       "      <td>t3_6uw3cv</td>\n",
       "      <td>t3_6uw3cv</td>\n",
       "      <td>This subject comes up in many forms and I can ...</td>\n",
       "      <td>1474</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5553</td>\n",
       "      <td>This subject comes up in many forms and I can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2630144</td>\n",
       "      <td>30079740149</td>\n",
       "      <td>t1_dtgp81h</td>\n",
       "      <td>1517320675</td>\n",
       "      <td>2comment</td>\n",
       "      <td>t3_7u0msk</td>\n",
       "      <td>t3_7u0msk</td>\n",
       "      <td>&gt;I don't understand her thought process, it's ...</td>\n",
       "      <td>1366</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>616</td>\n",
       "      <td>I don t understand her thought process it s li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2572373</td>\n",
       "      <td>29920059025</td>\n",
       "      <td>t1_dqtmpap</td>\n",
       "      <td>1512512112</td>\n",
       "      <td>bickisnotmyname</td>\n",
       "      <td>t3_7ht6tk</td>\n",
       "      <td>t3_7ht6tk</td>\n",
       "      <td>Haha. Wow lol 😝. I can’t believe what an idiot...</td>\n",
       "      <td>1284</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>262</td>\n",
       "      <td>Haha. Wow lol . I can t believe what an idiot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2151848</td>\n",
       "      <td>28973419429</td>\n",
       "      <td>t1_db60xid</td>\n",
       "      <td>1481679010</td>\n",
       "      <td>blacwidonsfw</td>\n",
       "      <td>t1_db5jjjy</td>\n",
       "      <td>t3_5i50pn</td>\n",
       "      <td>Lol if you get rejected at that point then you...</td>\n",
       "      <td>107</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>417</td>\n",
       "      <td>Lol if you get rejected at that point then you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>859738</td>\n",
       "      <td>27425728147</td>\n",
       "      <td>t1_clkkj9v</td>\n",
       "      <td>1414362203</td>\n",
       "      <td>Position5hero</td>\n",
       "      <td>t3_2kenbj</td>\n",
       "      <td>t3_2kenbj</td>\n",
       "      <td>Shoutout to the old granny in the comments sec...</td>\n",
       "      <td>107</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>634</td>\n",
       "      <td>Shoutout to the old granny in the comments sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1560210</td>\n",
       "      <td>28097868149</td>\n",
       "      <td>t1_cwoqtth</td>\n",
       "      <td>1446678660</td>\n",
       "      <td>TRPhd</td>\n",
       "      <td>t1_cwoqjmo</td>\n",
       "      <td>t3_3rjch9</td>\n",
       "      <td>Never, ever, under any circumstances, intentio...</td>\n",
       "      <td>107</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>510</td>\n",
       "      <td>Never ever under any circumstances intentional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>2208740</td>\n",
       "      <td>29076667233</td>\n",
       "      <td>t1_dcvhw0x</td>\n",
       "      <td>1485319263</td>\n",
       "      <td>dammit_redskins</td>\n",
       "      <td>t3_5q09dd</td>\n",
       "      <td>t3_5q09dd</td>\n",
       "      <td>I swear to christ if the progression of humani...</td>\n",
       "      <td>107</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>I swear to christ if the progression of humani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>2189376</td>\n",
       "      <td>29036230879</td>\n",
       "      <td>t1_dc7f74v</td>\n",
       "      <td>1483985509</td>\n",
       "      <td>Jester_13_</td>\n",
       "      <td>t3_5my6oj</td>\n",
       "      <td>t3_5my6oj</td>\n",
       "      <td>Shkreli is an interesting guy.  He taught hims...</td>\n",
       "      <td>107</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>720</td>\n",
       "      <td>Shkreli is an interesting guy. He taught himse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  ...                                         body_clean\n",
       "0        1650944  ...  If you marry this woman do not ever blame her ...\n",
       "1        2616308  ...  The scary thing about the Ansari case is that ...\n",
       "2        2457822  ...  This subject comes up in many forms and I can ...\n",
       "3        2630144  ...  I don t understand her thought process it s li...\n",
       "4        2572373  ...  Haha. Wow lol . I can t believe what an idiot ...\n",
       "...          ...  ...                                                ...\n",
       "9995     2151848  ...  Lol if you get rejected at that point then you...\n",
       "9996      859738  ...  Shoutout to the old granny in the comments sec...\n",
       "9997     1560210  ...  Never ever under any circumstances intentional...\n",
       "9998     2208740  ...  I swear to christ if the progression of humani...\n",
       "9999     2189376  ...  Shkreli is an interesting guy. He taught himse...\n",
       "\n",
       "[10000 rows x 13 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yj_uE4xZ-bdY"
   },
   "source": [
    "We can now use the Pandas `.apply` method, allowing us to apply a function along an axis of the DataFrame. We'll use a lambda function: a small stand-in function that can take arguments, but only one expression. This is what a lambda looks like. Do you see how it works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmgmLpMd9fF_"
   },
   "outputs": [],
   "source": [
    "df_com[\"body_clean\"] = df_com[\"body\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ok7Ham8-wa9"
   },
   "source": [
    "We now have an additional column in our DataFrame with cleaned up text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "7u2Z_-yO-GHJ",
    "outputId": "15c9ee98-7691-4a56-95f1-73b9c642d874"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>idint</th>\n",
       "      <th>idstr</th>\n",
       "      <th>created</th>\n",
       "      <th>author</th>\n",
       "      <th>parent</th>\n",
       "      <th>submission</th>\n",
       "      <th>body</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>distinguish</th>\n",
       "      <th>textlen</th>\n",
       "      <th>body_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1650944</td>\n",
       "      <td>28219675724</td>\n",
       "      <td>t1_cyp9l58</td>\n",
       "      <td>1452171945</td>\n",
       "      <td>Stories_of_Red</td>\n",
       "      <td>t3_3zv11k</td>\n",
       "      <td>t3_3zv11k</td>\n",
       "      <td>If you marry this woman, do not ever blame her...</td>\n",
       "      <td>1897</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>191</td>\n",
       "      <td>If you marry this woman do not ever blame her ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2616308</td>\n",
       "      <td>30034466365</td>\n",
       "      <td>t1_dspqukd</td>\n",
       "      <td>1516029389</td>\n",
       "      <td>KirthWGersen</td>\n",
       "      <td>t3_7qk2y3</td>\n",
       "      <td>t3_7qk2y3</td>\n",
       "      <td>The scary thing about the Ansari case is that ...</td>\n",
       "      <td>1776</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1297</td>\n",
       "      <td>The scary thing about the Ansari case is that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2457822</td>\n",
       "      <td>29621801610</td>\n",
       "      <td>t1_dlw20ei</td>\n",
       "      <td>1503252966</td>\n",
       "      <td>Thotwrecker</td>\n",
       "      <td>t3_6uw3cv</td>\n",
       "      <td>t3_6uw3cv</td>\n",
       "      <td>This subject comes up in many forms and I can ...</td>\n",
       "      <td>1474</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5553</td>\n",
       "      <td>This subject comes up in many forms and I can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2630144</td>\n",
       "      <td>30079740149</td>\n",
       "      <td>t1_dtgp81h</td>\n",
       "      <td>1517320675</td>\n",
       "      <td>2comment</td>\n",
       "      <td>t3_7u0msk</td>\n",
       "      <td>t3_7u0msk</td>\n",
       "      <td>&gt;I don't understand her thought process, it's ...</td>\n",
       "      <td>1366</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>616</td>\n",
       "      <td>I don t understand her thought process it s li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2572373</td>\n",
       "      <td>29920059025</td>\n",
       "      <td>t1_dqtmpap</td>\n",
       "      <td>1512512112</td>\n",
       "      <td>bickisnotmyname</td>\n",
       "      <td>t3_7ht6tk</td>\n",
       "      <td>t3_7ht6tk</td>\n",
       "      <td>Haha. Wow lol 😝. I can’t believe what an idiot...</td>\n",
       "      <td>1284</td>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>262</td>\n",
       "      <td>Haha. Wow lol . I can t believe what an idiot ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                         body_clean\n",
       "0     1650944  ...  If you marry this woman do not ever blame her ...\n",
       "1     2616308  ...  The scary thing about the Ansari case is that ...\n",
       "2     2457822  ...  This subject comes up in many forms and I can ...\n",
       "3     2630144  ...  I don t understand her thought process it s li...\n",
       "4     2572373  ...  Haha. Wow lol . I can t believe what an idiot ...\n",
       "\n",
       "[5 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_com.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KntxWVim-1K_"
   },
   "source": [
    "Let's turn it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FLjNJaCd3D17"
   },
   "outputs": [],
   "source": [
    "text_li = df_com['body_clean'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fYwCcPjN-7id"
   },
   "source": [
    "Next, we'll create a function that uses NLTK's `sent_tokenize()` method. This tokenizer splits our texts into sentences, which in turn are split into tokens. We'll also remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xEOgByps3aVx"
   },
   "outputs": [],
   "source": [
    "def sentence_tokenize(text):\n",
    "    sentence_doc = sent_tokenize(text)\n",
    "    sentences = [gensim.utils.simple_preprocess(str(doc), deacc=True) for doc in sentence_doc]  # deacc=True removes punctuations\n",
    "    stop = set(stopwords.words('english') + ['’', '“', '”', 'nbsp', 'http'])\n",
    "    no_stop = [[word for word in sentence if word not in stop] for sentence in sentences]\n",
    "    return no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k-8QFdsy3bPO"
   },
   "outputs": [],
   "source": [
    "com_sent_li = [sentence_tokenize(text) for text in text_li]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0svSRqk7v7Z"
   },
   "source": [
    "Note that we now have a list (of comments) of lists (sentences) of lists (tokens). Let's index the first token of the first sentence of the first comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "EgdIVhrt4VxE",
    "outputId": "1f8a749b-f515-4bef-a3c6-259e481b3be0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'marry'"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com_sent_li[0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmiEo5_66t16"
   },
   "source": [
    "We actually don't need the comment-level demarcation for the rest of our analysis. We can *flatten* our `com_sent_li` object to do so – this way, we create a list (of sentences) of lists (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVb8-KIU7_l-"
   },
   "outputs": [],
   "source": [
    "sent_li = []\n",
    "for sentence in com_sent_li:\n",
    "    for tokens in sentence:\n",
    "        sent_li.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5q9nQ2K08QGX"
   },
   "source": [
    "Writing the same in a list comprehension looks like this, by the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aPfdxQNr7PMi"
   },
   "outputs": [],
   "source": [
    "sent_li = [tokens for sentence in com_sent_li for tokens in sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p33QbO4G8kGs"
   },
   "source": [
    "Next, let's create a trigrams model using Gensim's `Phrases` and `Phraser` classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "QBZvKWy93ZWm",
    "outputId": "1b369748-7af5-4973-ff55-84d8c523c515"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "bigram = Phrases(sent_li, min_count=5, threshold=80)\n",
    "trigram = Phrases(bigram[sent_li], threshold=80)  \n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J65kFo458nJH"
   },
   "source": [
    "And let's run that model over our list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdZ5MqsG8mfF"
   },
   "outputs": [],
   "source": [
    "trigrams = [trigram_mod[bigram_mod[sentence]] for sentence in sent_li]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wk2mxX1HZOpk"
   },
   "source": [
    "## Word2Vec\n",
    "\n",
    "Let's create our word embeddings model. Its input is a text corpus (split up in sentences) and its output is a set of \"vectors\" in N dimensions. It allows us to group the vectors of similar words together in vectorspace. We can then reduce the dimensionality to visualize the results in a way humans can understand (such as in a 2-dimensional space), or to perform linear algebra in order to find how words are related.\n",
    "\n",
    "Word2vec is one example of a word embeddings model. It learns by taking words and their contexts (e.g. sentences) into account, and can then try to predict other words. Given enough data, usage and contexts, word2vec can make accurate guesses about a word’s meaning based on its appearances. Those guesses can be used to establish a word’s association with other words (e.g. “man” is to “boy” what “woman” is to “girl”), or cluster documents and classify them by topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ykxhE2K3Bqfj"
   },
   "source": [
    "### How many cores?\n",
    "Word2Vec can work using independent threads doing simultaneous training. In general, you'll never want to use more workers than the number of CPU cores you have in your machine. So let's check out how many you have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LN1nsJ7cO_oT",
    "outputId": "8119b096-c085-49a3-ab2a-1316e9002968"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in your computer\n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hIt-VOPQCQiP"
   },
   "source": [
    "We now instantiate and train our Word2Vec model, using the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sQq117-pZOpl"
   },
   "outputs": [],
   "source": [
    "num_features = 300        # Word vector dimensionality (how many features each word will be given)\n",
    "min_word_count = 2        # Minimum word count to be taken into account\n",
    "num_workers = cores       # Number of threads to run in parallel (equal to your amount of cores)\n",
    "context = 10              # Context window size\n",
    "downsampling = 1e-2       # Downsample setting for frequent words\n",
    "seed_n = 1                # Seed for the random number generator (to create reproducible results) \n",
    "sg_n = 1                  # Skip-gram = 1, CBOW = 0\n",
    "\n",
    "model = Word2Vec(trigrams, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, seed=seed_n, sg=sg_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrNV3eYeZOpo"
   },
   "source": [
    "That was it! We have a Word Embeddings model now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wWQC4S9IZOpu"
   },
   "source": [
    "How many terms are in our vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SegRmcybZOpw",
    "outputId": "f1d2e17e-ffed-47eb-89dc-1c7525f9bc73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16,814 terms in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "print('{:,} terms in the vocabulary.'.format(len(model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xw4th__0ZOp0"
   },
   "source": [
    "### Getting related terms\n",
    "\n",
    "With the information in our word embeddings model, we can try to find similarities between words that interest us (i.e. words that have a similar vector). Let's create a function that retrieves related terms to some input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v1HVouFqZOp0"
   },
   "outputs": [],
   "source": [
    "def get_related_terms(token, topn=20):\n",
    "    \"\"\"\n",
    "    look up the topn most similar terms to token and print them as a formatted list\n",
    "    \"\"\"\n",
    "\n",
    "    for word, similarity in model.most_similar(positive=[token], topn=topn):\n",
    "        print(word, round(similarity, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "CDyV-ntGZOp3",
    "outputId": "1192005f-b1af-4f8a-810b-bcfb7ef71b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman 0.669\n",
      "expects 0.641\n",
      "queen 0.632\n",
      "worthy 0.628\n",
      "submit 0.626\n",
      "deserves 0.625\n",
      "loves 0.62\n",
      "strong_independent 0.616\n",
      "desired 0.615\n",
      "fails 0.611\n",
      "youth 0.607\n",
      "loyal 0.606\n",
      "passionate 0.604\n",
      "charming 0.603\n",
      "dude 0.603\n",
      "chooses 0.602\n",
      "single_mother 0.602\n",
      "lonely 0.601\n",
      "learns 0.601\n",
      "guy 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "get_related_terms(u'man')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LAF3aRsHZOp6"
   },
   "source": [
    "### Word algebra\n",
    "\n",
    "Word algebra, also known as analogy completion, means doing math with words (like the famous example \"king - man + woman = queen\". The core idea is that once words are represented as numerical vectors, you can do math with them. The mathematical procedure works as follows:\n",
    "\n",
    "1. Provide a set of words or phrases you want to add or subtract.\n",
    "2. Look up the vectors that represent those terms in the word vector model.\n",
    "3. Add and subtract those vectors to produce a new, combined vector.\n",
    "4. Look up the most similar vector(s) to this new, combined vector via cosine similarity.\n",
    "5. Return the word(s) associated with the similar vector(s).\n",
    "\n",
    "Let's try it out. We'll create a function that does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8RsoWBAZOp7"
   },
   "outputs": [],
   "source": [
    "def word_algebra(add=[], subtract=[], topn=1):\n",
    "    \"\"\"\n",
    "    combine the vectors associated with the words provided\n",
    "    in add= and subtract=, look up the topn most similar\n",
    "    terms to the combined vector, and print the result(s)\n",
    "    \"\"\"\n",
    "    answers = model.most_similar(positive=add, negative=subtract, topn=topn)\n",
    "    \n",
    "    for term, similarity in answers:\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "hXhZB8SOZOp9",
    "outputId": "a7d11ae9-b405-449f-9f8d-7e895759a751",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ltrs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "word_algebra(add=['game','dating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "ho-F51R3ZOqB",
    "outputId": "7d6a1cc2-8199-45d2-89b6-f5f89f3a2aa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "word_algebra(add=['game', 'dating'], subtract=['beta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L6Y4eXciZOqb"
   },
   "source": [
    "## K-means clustering (advanced)\n",
    "One convenience of word embeddings is that we can cluster them using, for instance, K-Means clustering. Don't worry if you don't understand all of the following, just check out how it works!\n",
    "\n",
    "K-Means clustering aims to partition N observations into K clusters in which each observation belongs to the cluster with the nearest mean (called the \"cluster centre\"), which serves as a prototype of the cluster.\n",
    "\n",
    "Since our words are all represented as vectors, applying K-Means is easy to do since the clustering algorithm will simply look at differences between vectors (and centers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rJodQkitZOqb"
   },
   "outputs": [],
   "source": [
    "def clustering_on_wordvecs(word_vectors, num_clusters):\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++');\n",
    "    idx = kmeans_clustering.fit_predict(word_vectors);\n",
    "    return kmeans_clustering.cluster_centers_, idx;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qPRtxyZfZOqd"
   },
   "outputs": [],
   "source": [
    "Z = model.wv.syn0 # The syn0 array essentially holds raw word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Em2CvpgRZOqf"
   },
   "outputs": [],
   "source": [
    "centers, clusters = clustering_on_wordvecs(Z, 10);\n",
    "centroid_map = dict(zip(model.wv.index2word, clusters));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15_523YKZOqj"
   },
   "source": [
    "Next, we get words in each cluster that are closest to the cluster center. To do this, we initialize a KDTree on the word vectors, and query it for the Top K words on each cluster center. Using the Index 2 word dictionary, we than correspond each word vector back to it’s original word representation and add them to a dataframe for easier printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O9aD76xcZOqk"
   },
   "outputs": [],
   "source": [
    "def get_top_words(index2word, k, centers, wordvecs):\n",
    "    tree = KDTree(wordvecs);\n",
    "    # Use closest points for each cluster center to query closest 20 points to it\n",
    "    closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers];\n",
    "    closest_words_idxs = [x[1] for x in closest_points];\n",
    "    # Query Word Index  for each position in the above array, and added to a Dictionary\n",
    "    closest_words = {};\n",
    "    for i in range(0, len(closest_words_idxs)):\n",
    "        closest_words['Cluster #' + str(i)] = [index2word[j] for j in closest_words_idxs[i][0]]\n",
    "    # Create DataFrame from dictionary\n",
    "    df = pd.DataFrame(closest_words);\n",
    "    df.index = df.index+1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OtBFZL_HZOqm"
   },
   "source": [
    "Let’s get the top words and print the first 20 in each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SwzjSQJlZOqn"
   },
   "outputs": [],
   "source": [
    "top_words = get_top_words(model.wv.index2word, 5000, centers, Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "tI19jNzsZOqq",
    "outputId": "7d7a117e-80ec-4188-a944-907aedb86c64",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cluster #0</th>\n",
       "      <th>Cluster #1</th>\n",
       "      <th>Cluster #2</th>\n",
       "      <th>Cluster #3</th>\n",
       "      <th>Cluster #4</th>\n",
       "      <th>Cluster #5</th>\n",
       "      <th>Cluster #6</th>\n",
       "      <th>Cluster #7</th>\n",
       "      <th>Cluster #8</th>\n",
       "      <th>Cluster #9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fortress</td>\n",
       "      <td>alpha_widow</td>\n",
       "      <td>wider</td>\n",
       "      <td>prospects</td>\n",
       "      <td>sexual_marketplace</td>\n",
       "      <td>insightful</td>\n",
       "      <td>climbs</td>\n",
       "      <td>cooked</td>\n",
       "      <td>sandals</td>\n",
       "      <td>punching_bag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>overreaction</td>\n",
       "      <td>picky</td>\n",
       "      <td>ai</td>\n",
       "      <td>steal</td>\n",
       "      <td>elevate</td>\n",
       "      <td>commenters</td>\n",
       "      <td>puke</td>\n",
       "      <td>cocks</td>\n",
       "      <td>rats</td>\n",
       "      <td>cautious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>butthole</td>\n",
       "      <td>heh</td>\n",
       "      <td>sciences</td>\n",
       "      <td>worthwhile</td>\n",
       "      <td>involves</td>\n",
       "      <td>depths</td>\n",
       "      <td>tracks</td>\n",
       "      <td>min</td>\n",
       "      <td>predicted</td>\n",
       "      <td>leveraged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kbbg</td>\n",
       "      <td>emotional_tampon</td>\n",
       "      <td>transferring</td>\n",
       "      <td>scarcity</td>\n",
       "      <td>pragmatic</td>\n",
       "      <td>tho</td>\n",
       "      <td>wandering</td>\n",
       "      <td>wakes</td>\n",
       "      <td>vulgar</td>\n",
       "      <td>failings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stayin</td>\n",
       "      <td>mess</td>\n",
       "      <td>structures</td>\n",
       "      <td>hassle</td>\n",
       "      <td>nurture</td>\n",
       "      <td>agreeing</td>\n",
       "      <td>handy</td>\n",
       "      <td>midnight</td>\n",
       "      <td>affectionate</td>\n",
       "      <td>inconsistent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>outstanding</td>\n",
       "      <td>cuz</td>\n",
       "      <td>depressants</td>\n",
       "      <td>chores</td>\n",
       "      <td>principle</td>\n",
       "      <td>helpful</td>\n",
       "      <td>abstained</td>\n",
       "      <td>truck</td>\n",
       "      <td>slowing</td>\n",
       "      <td>cake_eat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fetuses</td>\n",
       "      <td>enjoys</td>\n",
       "      <td>sci</td>\n",
       "      <td>declining</td>\n",
       "      <td>undermine</td>\n",
       "      <td>misunderstood</td>\n",
       "      <td>shoved</td>\n",
       "      <td>waited</td>\n",
       "      <td>pains</td>\n",
       "      <td>crudely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nucleus</td>\n",
       "      <td>shrug</td>\n",
       "      <td>jurisprudence</td>\n",
       "      <td>lifelong</td>\n",
       "      <td>conflict</td>\n",
       "      <td>analogy</td>\n",
       "      <td>crosshairs</td>\n",
       "      <td>boot</td>\n",
       "      <td>duality</td>\n",
       "      <td>situational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>swimmers</td>\n",
       "      <td>fooled</td>\n",
       "      <td>dogma</td>\n",
       "      <td>companionship</td>\n",
       "      <td>unwilling</td>\n",
       "      <td>askwomen</td>\n",
       "      <td>brad</td>\n",
       "      <td>crawling</td>\n",
       "      <td>nutjobs</td>\n",
       "      <td>victimization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dissolve</td>\n",
       "      <td>scumbag</td>\n",
       "      <td>canadian</td>\n",
       "      <td>prioritize</td>\n",
       "      <td>feminine_imperative</td>\n",
       "      <td>mentioning</td>\n",
       "      <td>borrow</td>\n",
       "      <td>lb</td>\n",
       "      <td>researched</td>\n",
       "      <td>condemns</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cluster #0        Cluster #1  ...    Cluster #8     Cluster #9\n",
       "1       fortress       alpha_widow  ...       sandals   punching_bag\n",
       "2   overreaction             picky  ...          rats       cautious\n",
       "3       butthole               heh  ...     predicted      leveraged\n",
       "4           kbbg  emotional_tampon  ...        vulgar       failings\n",
       "5         stayin              mess  ...  affectionate   inconsistent\n",
       "6    outstanding               cuz  ...       slowing       cake_eat\n",
       "7        fetuses            enjoys  ...         pains        crudely\n",
       "8        nucleus             shrug  ...       duality    situational\n",
       "9       swimmers            fooled  ...       nutjobs  victimization\n",
       "10      dissolve           scumbag  ...    researched       condemns\n",
       "\n",
       "[10 rows x 10 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words[:10]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "LAF3aRsHZOp6",
    "liXCdKelZOqE",
    "5p4obNSvZOqG",
    "L6Y4eXciZOqb",
    "p6cjf2BYZOqt"
   ],
   "name": "Week 4-3 Word Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
