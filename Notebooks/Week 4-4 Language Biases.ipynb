{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Week 4-4 Language Biases.ipynb","provenance":[{"file_id":"1HXx9kevQetMI9pE0B9XCTFjgUHiGC4hm","timestamp":1594637500809}],"collapsed_sections":[]},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TtIBMFnNrQlL","colab_type":"text"},"source":["<div style=\"display: block; width: 100%; height: 120px;\">\n","\n","<p style=\"float: left;\">\n","    <span style=\"font-weight: bold; line-height: 24px; font-size: 16px;\">\n","        DIGHUM160 - Critical Digital Humanities\n","        <br />\n","        Digital Hermeneutics 2020\n","    </span>\n","    <br >\n","    <span style=\"line-height: 22x; font-size: 14x; margin-top: 10px;\">\n","        Week 4-4: Biases in word embeddings<br />\n","        Created by Xavier Ferrer (xavier.ferrer.aran@kcl.ac.uk) and Tom van Nuenen (tom.van_nuenen@kcl.ac.uk)<br />\n","    </span>\n","</p>\n","\n","# Discovering Language Biases with Word Embeddings\n","As we have seen in the past weeks, language carries implicit biases, functioning both as a reflection and a perpetuation of stereotypes that people carry with them. We saw how that rule applies to Word Embeddings language models as well.\n","\n","We can make these biases the focus of our research. Using word embeddings, we can postulate concepts such as \"male\" or \"female\", both of which include a number of word vectors. Using these so-called *target concepts*, we can then \n","compute relative similarities of other word vectors – particularly, words that act as evaluative attributes such as \"strong\" and \"sensitive\". \n","\n","These words can be categorised through clustering algorithms and labeled through a semantic analysis system into more general (conceptual) biases, yielding a broad picture of the biases present in a discourse community.\n","\n","See https://xfold.github.io/WE-GenderBiasVisualisationWeb/ for a web demo. Today, we will create a similar model for our own data.\n"]},{"cell_type":"markdown","metadata":{"id":"te8zJYVLrQlO","colab_type":"text"},"source":["## Get the data\n","Replace this with your own data! I suggest using the \"comments\" CSV, as it tends to be larger."]},{"cell_type":"code","metadata":{"id":"p0AkPRZJ0MiS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596263941081,"user_tz":-180,"elapsed":1076,"user":{"displayName":"Tom van Nuenen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64","userId":"10012302451096885058"}}},"source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LaEjctJwtPnI","colab":{},"executionInfo":{"status":"ok","timestamp":1596263965714,"user_tz":-180,"elapsed":2864,"user":{"displayName":"Tom van Nuenen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64","userId":"10012302451096885058"}}},"source":["downloaded = drive.CreateFile({'id':\"1nY9JtXoGJa7B-OmU6afh4qfcFGPCQIHW\"})   \n","downloaded.GetContentFile('TRP-comments.csv')"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TShMBIgV23jf","colab_type":"text"},"source":["## Training a WE model\n","\n","First, we need to train our Word Embeddings model. We create a function that takes in a CSV file and applies Gensim's `simple_preprocess` method on the \"body\" column (swap this for \"selftext\" if you're using a submissions.csv file).It also lemmatizes the data if we want, and finally creates a Word2Vec model with parameters we can feed into the function."]},{"cell_type":"code","metadata":{"id":"VZ6k3qaS0CbE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596263971837,"user_tz":-180,"elapsed":2380,"user":{"displayName":"Tom van Nuenen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64","userId":"10012302451096885058"}}},"source":["import math\n","from textblob import TextBlob as tb\n","import nltk\n","from nltk.corpus import stopwords\n","import re\n","import gensim \n","import pandas as pd\n","import logging\n","import os\n","import time\n","from nltk.stem import WordNetLemmatizer\n","wordnet_lemmatizer = WordNetLemmatizer()\n","\n","def train_model(csv_document, csv_comment_column='body', outputname='output_model', window = 4, minf=10, epochs=100, ndim=200, lemmatiseFirst = False):\n","    '''\n","    Load the documents from document_l, a list of sentences, and train a WE model with specified\n","    minf, epochs and ndims. where:\n","    csv_document : csv document containing all information, where each comment is on a different row\n","    csv_comment_column : name of the column taht contains the text we want to process\n","    outputname : output path of the resulting model\n","    \n","    returns\n","    path of the trained models\n","    '''\n","    \n","    def preprocess_csv(path, column = 'body_clean', nrowss=None):\n","        df_com = pd.read_csv(path, lineterminator='\\n', nrows=nrowss)\n","\n","        documents = []\n","        for i, row in enumerate(df_com[column]):\n","            if i%500000 == 0:\n","                print('\\t...processing line {}'.format(i))\n","            try:\n","                pp = gensim.utils.simple_preprocess (row)\n","                if(lemmatiseFirst == True):\n","                    pp = [wordnet_lemmatizer.lemmatize(w, pos=\"n\") for w in pp]\n","                documents.append(pp)\n","            except:\n","                print('\\terror with row {}'.format(row))\n","\n","        logging.info (\"Done reading and preprocessing data file {} \".format(path))\n","        return documents\n","\n","    def train_WE_model(documents, outputfile, ndim, window, minfreq, epochss):\n","        '''\n","        size\n","        The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller\n","        value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me.\n","\n","        window\n","        The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum \n","        window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a \n","        smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too \n","        much, as long as its a decent sized window.\n","\n","        min_count\n","        Minimium frequency count of words. The model would ignore words that do not statisfy the min_count. Extremely infrequent words are \n","        usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n","\n","        workers\n","        How many threads to use behind the scenes?\n","        '''\n","        starttime = time.time()\n","        print('->->Starting training model {} with dimensions:{}, minf:{}, epochs:{}'.format(outputfile,ndim, minfreq, epochss))\n","        model = gensim.models.Word2Vec (documents, size=ndim, window=window, min_count=minfreq, workers=5)\n","        model.train(documents,total_examples=len(documents),epochs=epochss)\n","        model.save(outputfile)\n","        print('->-> Model saved in {}'.format(outputfile))\n","    \n","    print('->Starting with {} [{}], output {}, window {}, minf {}, epochs {}, ndim {}'.format(csv_document, \n","                                                                                       csv_comment_column,\n","                                                                                       outputname, window, minf, epochs, ndim))\n","    docs = preprocess_csv(csv_document, csv_comment_column, None)\n","    starttime = time.time()\n","    ofile = outputname\n","    print('-> Output will be saved in {}'.format(ofile))\n","    train_WE_model(docs, ofile, ndim, window, minf, epochs)\n","    print('-> Model creation ended in {} seconds'.format(time.time()-starttime))"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p9NtlPTtQucJ","colab_type":"text"},"source":["This function has been created to run over different CSV's and using different parameters (like we did with our topic models). Below, we create a `training_setup` dictionary that can include multiple CSV files and parameters. This makes it a bit easier to replicate the process. For now, we've entered one CSV file: the one we have loaded. We will save the output of our function – the Word Embeddings model – in a file as well."]},{"cell_type":"code","metadata":{"id":"VgJPLcKjrQlV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"executionInfo":{"status":"ok","timestamp":1596264121223,"user_tz":-180,"elapsed":151049,"user":{"displayName":"Tom van Nuenen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64","userId":"10012302451096885058"}},"outputId":"9ebec5af-8f9f-4444-e305-f01aef49f4d2"},"source":["training_setup = [\n","    {'csvfile': \"TRP-comments.csv\", 'output_file': 'trp_w4_f10_e100_d200.model', 'w':4, 'minf': 10, 'epochs':100 ,'ndim':200}\n","]\n","\n","for setup in training_setup:\n","    train_model(setup['csvfile'], \n","        'body',\n","        outputname = setup['output_file'],\n","        window = setup['w'],\n","        minf = setup['minf'],\n","        epochs = setup['epochs'],\n","        ndim = setup['ndim'],\n","        )"],"execution_count":12,"outputs":[{"output_type":"stream","text":["->Starting with TRP-comments.csv [body], output trp_w4_f10_e100_d200.model, window 4, minf 10, epochs 100, ndim 200\n","\t...processing line 0\n","-> Output will be saved in trp_w4_f10_e100_d200.model\n","->->Starting training model trp_w4_f10_e100_d200.model with dimensions:200, minf:10, epochs:100\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n"],"name":"stderr"},{"output_type":"stream","text":["->-> Model saved in trp_w4_f10_e100_d200.model\n","-> Model creation ended in 148.24260187149048 seconds\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"FSbUVbC-rQlY","colab_type":"text"},"source":["## Load Model and get biased words\n","\n","We now run our method of finding biased words towards our target sets.\n","\n","Given a vocabulary and two sets of target words (such as, in this case, those for *women* and *men*, we rank the words from least to most biased. As such, we obtain two ordered lists of the most biased words towards each target set, obtaining an overall view of the bias distribution in that particular community with respect to those two target sets. \n","\n","Here's what happening in the next block of code:\n","- We calculate the centroid of a target set by averaging the embedding vectors in our target set (e.g. the vectors for `he, son, his, him, father, male` for our target concept `male`);\n","- We calculate the cosine similarity between the vectors for all words in our vocabulary as compared to our two centroids (we also apply POS-filtering to only work with parts of speech we expect to be relevant);\n","- We use a threshold based on standard deviation to determine how severe a bias needs to be before we include it;\n","- We rank the words in the vocabulary of our Word Embeddings model based on their bias towards either target concept.\n"]},{"cell_type":"code","metadata":{"id":"4AHqdHpSrQlZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1596264122355,"user_tz":-180,"elapsed":151757,"user":{"displayName":"Tom van Nuenen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64","userId":"10012302451096885058"}},"outputId":"2a54ba65-00be-4255-a419-0f2328175fcc"},"source":["from gensim.models import Word2Vec\n","from gensim.test.utils import datapath, get_tmpfile\n","from gensim.models import KeyedVectors\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","from operator import itemgetter\n","from scipy import spatial\n","import nltk\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('vader_lexicon')\n","import inflect\n","import numpy as np\n","import statistics\n","import json\n","import itertools\n","\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from datetime import datetime\n","import statistics\n","\n","def _calculate_centroid(model, wordlist):\n","    '''\n","    Calculate centroid of the wordlist list of words based on the model embedding vectors\n","    '''\n","    centr = np.zeros( len(model.wv[wordlist[0]]) )\n","    for w in wordlist:\n","        centr += np.array(model.wv[w])\n","    return centr/len(wordlist)\n","\n","def _keep_only_model_words(model, words):\n","    aux = [ word for word in words if word in model.wv.vocab.keys()]\n","    return aux\n","\n","def _get_word_freq(model, word):\n","    if word in model.wv.vocab:\n","        wm = model.wv.vocab[word]\n","        return [word, wm.count, wm.index]\n","    return None\n","\n","def _get_model_min_max_rank(model):\n","    minF = 999999\n","    maxF = -1\n","    for w in model.wv.vocab:\n","        wm = model.wv.vocab[w] #wm.count, wm.index\n","        rank = wm.index\n","        if(minF>rank):\n","            minF = rank\n","        if(maxF<rank):\n","            maxF = rank\n","    return [minF, maxF]\n","\n","sid = SentimentIntensityAnalyzer()\n","def _get_sentiment(word):\n","    return sid.polarity_scores(word)['compound']\n","\n","'''\n","Normalises a value in the positive space\n","'''    \n","def _normalise(val, minF, maxF):\n","    #print(val, minF, maxF)\n","    if(maxF<0 or minF<0 or val<0):\n","        raise Exception('All values should be in the positive space. minf: {}, max: {}, freq: {}'.format(minF, maxF, val))\n","    if(maxF<= minF):\n","        raise Exception('Maximum frequency should be bigger than min frequency. minf: {}, max: {}, freq: {}'.format(minF, maxF, freq))\n","    val -= minF\n","    val = val/(maxF-minF)\n","    return val\n","\n","def _get_cosine_distance(wv1, wv2):\n","    return spatial.distance.cosine(wv1, wv2)\n","\n","def _get_min_max(dict_value):\n","    l = list(dict_value.values())\n","    return [ min(l), max(l)]\n","\n","def _find_stdev_threshold_sal(dwords, stdevs):\n","    '''\n","    dword is an object like {'word':w, 'bias':bias, 'biasW':biasW, 'freq':freq, 'freqW':freqW, 'sal':val, 'wv':wv, 'sent':sent }\n","    stdevs : minimum stdevs for which we want to compute the threshold\n","\n","    returns\n","    outlier_thr : the threshold correpsonding to stdevs considering salience values from the dwrods object list\n","    '''\n","    allsal = []\n","    for obj in dwords:\n","        allsal.append(obj['sal'])\n","    stdev = statistics.stdev(allsal)\n","    outlier_thr = (stdev*stdevs)+sum(allsal)/len(allsal)\n","    return outlier_thr\n","\n","def calculate_biased_words(model, targetset1, targetset2, stdevs, \n","                         acceptedPOS = ['JJ', 'JJS', 'JJR','NN', 'NNS', 'NNP', 'NNPS','VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ' ], \n","                         words = None, force=False):\n","    '''\n","    this function calculates the list of biased words towards targetset1 and taregset2 with salience > than the \n","    specified times (minstdev) of standard deviation.\n","\n","    targetset1 <list of strings> : target set 1\n","    targetset2 <list of strings> : target set 2\n","    minstdev int : Minium threhsold for stdev to select biased words\n","    acceptedPOS <list<str>> : accepted list of POS to consider for the analysis, as defined in NLTK POS tagging lib. \n","                              If None, no POS filtering is applied and all words in the vocab are considered\n","    words list<str> : list of words we want to consider. If None, all words in the vocab are considered\n","    '''\n","    if(model is None):\n","        raise Exception(\"You need to define a model to estimate biased words.\")\n","    if(targetset1 is None or targetset2 is None):\n","        raise Exception(\"Target sets are necessary to estimate biased words.\")\n","    if(stdevs is None):\n","        raise Exception(\"You need to define a minimum threshold for standard deviation to select biased words.\")\n","   \n","    tset1 = _keep_only_model_words(model, targetset1) # remove target set words that do not exist in the model\n","    tset2 = _keep_only_model_words(model, targetset2) # remove target set words that do not exist in the model\n","\n","    # We remove words in the target sets, and also their plurals from the set of interesting words to process.\n","    engine = inflect.engine()\n","    toremove = targetset1 + targetset2 + [engine.plural(w) for w in targetset1] + [engine.plural(w) for w in targetset2]\n","    if(words is None):\n","        words = [w for w in model.wv.vocab.keys() if w not in toremove]\n","\n","    # Calculate centroids \n","    tset1_centroid = _calculate_centroid(model, tset1)\n","    tset2_centroid = _calculate_centroid(model, tset2)\n","    [minR, maxR] = _get_model_min_max_rank(model)\n","\n","    # Get biases for words\n","    biasWF = {}\n","    biasWM = {}\n","    for i, w in enumerate(words):\n","        p = nltk.pos_tag([w])[0][1]\n","        if acceptedPOS is not None and p not in acceptedPOS:\n","            continue\n","        wv = model.wv[w]\n","        diff = _get_cosine_distance(tset2_centroid, wv) - _get_cosine_distance(tset1_centroid, wv)\n","        if(diff>0):\n","            biasWF[w] = diff\n","        else:\n","            biasWM[w] = -1*diff\n","\n","    # Get min and max bias for both target sets, so we can normalise these values later\n","    [minbf, maxbf] = _get_min_max(biasWF)\n","    [minbm, maxbm] = _get_min_max(biasWM)\n","\n","    # Iterate through all 'selected' words\n","    biased1 = []\n","    biased2 = []\n","    for i, w in enumerate(words):\n","        # Print('..Processing ', w)\n","        p = nltk.pos_tag([w])[0][1]\n","        if acceptedPOS is not None and p not in acceptedPOS:\n","            continue\n","        wv = model.wv[w]\n","        # Sentiment\n","        sent = _get_sentiment(w)\n","        # Rank and rank norm\n","        freq = _get_word_freq(model, w)[1]\n","        rank = _get_word_freq(model, w)[2]\n","        rankW = 1-_normalise(rank, minR, maxR) \n","\n","        # Normalise bias\n","        if(w in biasWF):\n","            bias = biasWF[w]\n","            biasW = _normalise(bias, minbf, maxbf)\n","            val = biasW * rankW\n","            biased1.append({'word':w, 'bias':bias, 'biasW':biasW, 'freq':freq, 'rank':rank, 'rankW':rankW, 'sal':val, 'wv':wv.tolist(), 'sent':sent } ) \n","        if(w in biasWM):\n","            bias = biasWM[w]\n","            biasW = _normalise(bias, minbm, maxbm)\n","            val = biasW * rankW\n","            biased2.append({'word':w, 'bias':bias, 'biasW':biasW, 'freq':freq, 'rank':rank, 'rankW':rankW, 'sal':val, 'wv':wv.tolist(), 'sent':sent } ) \n","\n","    # Calculate the salience threshold for both word sets, and select the list of biased words (i.e., which words do we discard?)\n","    stdevs1_thr = _find_stdev_threshold_sal(biased1, stdevs)\n","    stdevs2_thr = _find_stdev_threshold_sal(biased2, stdevs)\n","    # biased1.sort(key=lambda x: x['sal'], reverse=True)\n","    b1_dict = {}\n","    for k in biased1:\n","        if(k['sal']>=stdevs1_thr):\n","            b1_dict[k['word']] = k\n","    # biased2.sort(key=lambda x: x['sal'], reverse=True)\n","    b2_dict = {}\n","    for k in biased2:\n","        if(k['sal']>=stdevs2_thr):\n","            b2_dict[k['word']] = k\n","\n","    #transform centroid tol list so they become serializable\n","    tset1_centroid = tset1_centroid.tolist() \n","    tset2_centroid = tset2_centroid.tolist()\n","    return [b1_dict, b2_dict]"],"execution_count":13,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n","  warnings.warn(\"The twython library has not been installed. \"\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T7wSXVuJrQld","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1596264122357,"user_tz":-180,"elapsed":151572,"user":{"displayName":"Tom van Nuenen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64","userId":"10012302451096885058"}},"outputId":"fda98145-51af-4a65-d4ba-b67842124644"},"source":["modelpath = \"trp_w4_f10_e100_d200.model\"\n","\n","model = Word2Vec.load(modelpath)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"aan7eV3SqhkZ","colab_type":"text"},"source":["Here we create the two target sets, called `t1` and `t2`. These two lists are the ones you'll want to swap out if you are going to create your own target sets to find biases!"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"SIzdtQV5rQlh","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596264125373,"user_tz":-180,"elapsed":154043,"user":{"displayName":"Tom van Nuenen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64","userId":"10012302451096885058"}}},"source":["t1=[\"sister\" , \"female\" , \"woman\" , \"girl\" , \"daughter\" , \"she\" , \"hers\" , \"her\"]\n","t2=[\"brother\" , \"male\" , \"man\" , \"boy\" , \"son\" , \"he\" , \"his\" , \"him\"] \n","\n","[b1, b2] = calculate_biased_words(model, t1, t2, 4)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LJpg5BDydlYT","colab_type":"text"},"source":["Let's print some biases. Here you see the most-biased words towards our target concepts (1 being *women*, 2 being *men*)."]},{"cell_type":"code","metadata":{"id":"qc-9g8BErQll","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1596264125375,"user_tz":-180,"elapsed":153596,"user":{"displayName":"Tom van Nuenen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64","userId":"10012302451096885058"}},"outputId":"39a3c15d-8f5f-4348-8595-398a0521ebd7"},"source":["print('Biased words towards target set 1')\n","print( [w for w in b1.keys()] )\n","print()\n","print('Biased words towards target set 2')\n","print( [w for w in b2.keys()] )"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Biased words towards target set 1\n","['way', 'food', 'person', 'truth', 'hamster', 'chick', 'chicks', 'number', 'plate']\n","\n","Biased words towards target set 2\n","['was', 'had', 'became', 'proud', 'wish', 'helps', 'killed', 'young', 'went', 'worked', 'thanks', 'taught', 'asshole', 'father', 'raised']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qvav-HBtrQlp","colab_type":"text"},"source":["## Clustering similar words (K-means + silhouette)\n","\n","Here,  we group our language biases in more general clusters. We do so using the K-means clustering algorithm, and use silhouette scores to validate the consistency within our clusters of data. \n","\n","In general, this results in words with similar meanings being clustered together. Clustering allows the biased words to be better interpretable, as their context becomes clearer. "]},{"cell_type":"code","metadata":{"id":"ZbG2pMTgrQlq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1596264125376,"user_tz":-180,"elapsed":153194,"user":{"displayName":"Tom van Nuenen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64","userId":"10012302451096885058"}},"outputId":"66db6d0e-8b83-4e3c-8954-832b0a7ddb1c"},"source":["import pandas as pd\n","import gensim\n","import nltk.data\n","import numpy as np\n","from scipy import spatial\n","from sklearn.cluster import KMeans\n","import sklearn\n","\n","'''\n","TARGET SET 1\n","'''\n","t1_embeddings = [b1[w]['wv'] for w in b1] # t1 embeddings = list of embeddings of words biased towards target set 1\n","t1_words = [w for w in b1.keys()]\n","\n","# Clustering\n","rangek = range(2, int((len(t1_embeddings)/2)-1) ) # Clusters should be min size 2 at max half of the amount of words (speeding up + forcing clusters)\n","print('[Testing', rangek, 'clusters]')\n","kmeans_p = [ KMeans(n_clusters=k).fit_predict(t1_embeddings) for k in rangek] \n","kmeans_sil = [ sklearn.metrics.silhouette_score(t1_embeddings, labels) for labels in kmeans_p] \n","print('[Silhouette values', kmeans_sil)\n","indexmaxsil =  kmeans_sil.index(max(kmeans_sil))\n","print('[Max silhouette, ', max(kmeans_sil), '; index_k: ',indexmaxsil,']')\n","\n","# Aggregating all clusters from same index in list\n","clusters1 = {}\n","for i, index in enumerate(kmeans_p[indexmaxsil]): # returns list of cluster index, telling you which cluster each word belongs to \n","    if(index in clusters1):\n","        clusters1[index].append(t1_words[i])\n","    else:\n","        clusters1[index]  = [t1_words[i]]\n","        \n","        \n","'''\n","TARGET SET 2\n","'''\n","t2_embeddings = [b2[w]['wv'] for w in b2]\n","t2_words = [w for w in b2.keys()]\n","\n","# Clustering\n","rangek = range(2, int((len(t2_embeddings)/2)-1) )\n","print('[Testing', rangek, 'clusters]')\n","kmeans_p = [ KMeans(n_clusters=k).fit_predict(t2_embeddings) for k in rangek ] \n","kmeans_sil = [ sklearn.metrics.silhouette_score(t2_embeddings, labels) for labels in kmeans_p] \n","print('[Silhouette values', kmeans_sil)\n","indexmaxsil =  kmeans_sil.index(max(kmeans_sil))\n","print('[Max silhouette, ', max(kmeans_sil), '; index_k: ',indexmaxsil,']')\n","\n","clusters2 = {}\n","for i, index in enumerate(kmeans_p[indexmaxsil]):\n","    if(index in clusters2):\n","        clusters2[index].append(t2_words[i])\n","    else:\n","        clusters2[index]  = [t2_words[i]]"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[Testing range(2, 3) clusters]\n","[Silhouette values [0.0770947501414199]\n","[Max silhouette,  0.0770947501414199 ; index_k:  0 ]\n","[Testing range(2, 6) clusters]\n","[Silhouette values [0.06607639150112862, 0.06613106281579811, 0.06990627054335712, 0.0677788297607422]\n","[Max silhouette,  0.06990627054335712 ; index_k:  2 ]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FKIP7T9u0prK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596267292821,"user_tz":-180,"elapsed":1520,"user":{"displayName":"Tom van Nuenen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64","userId":"10012302451096885058"}},"outputId":"e32fe22e-7085-4d5e-8a0e-254e091f0791"},"source":["Y = clusters1.values()\n","Y"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_values([['way', 'person', 'truth', 'hamster', 'chick', 'chicks', 'number', 'plate'], ['food']])"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"b5lZ4_1WeYr9","colab_type":"text"},"source":["Let's print the clusters we've got."]},{"cell_type":"code","metadata":{"id":"CIXYxKM9eB5z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1596264125378,"user_tz":-180,"elapsed":150563,"user":{"displayName":"Tom van Nuenen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64","userId":"10012302451096885058"}},"outputId":"236466e0-a40f-48d1-c8a7-54c3ee553454"},"source":["print('Clusters target set 1')\n","print( list( clusters1.values()) )        \n","\n","print('Clusters target set 2')\n","print( list( clusters2.values()) )        "],"execution_count":18,"outputs":[{"output_type":"stream","text":["Clusters target set 1\n","[['way', 'person', 'truth', 'hamster', 'chick', 'chicks', 'number', 'plate'], ['food']]\n","Clusters target set 2\n","[['was', 'had', 'became', 'went', 'worked', 'thanks'], ['proud', 'asshole', 'father'], ['wish', 'helps', 'killed', 'young'], ['taught', 'raised']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p4XnvP_1Wml4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596267038348,"user_tz":-180,"elapsed":201636,"user":{"displayName":"Tom van Nuenen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64","userId":"10012302451096885058"}}},"source":["from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","\n","X = model.wv[model.wv.vocab]\n","tsne = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n","X_tsne = tsne.fit_transform(X)\n","\n","# PLOTTING - CHANGE THIS\n","\n","plt.figure(figsize=(16, 16)) \n","for i in range(len(x)):\n","    plt.scatter(x[i],y[i])\n","    plt.annotate(labels[i],\n","                  xy=(x[i], y[i]),\n","                  xytext=(5, 2),\n","                  textcoords='offset points',\n","                  ha='right',\n","                  va='bottom')\n","plt.show()"],"execution_count":37,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AW0lJqLpqayn","colab_type":"text"},"source":["## Assignment: Creating your own target sets\n","\n","For this week's assignment, try to expose the biases of your own reddit dataset. You can use the target sets defined below, but also try to create your own. For instance, if you'd want to see which words are biased towards the political left and right, you could create two target sets \"Left\" and \"Right\" with respective attributes such as `left-wing, leftist, progressive`, and `right-wing, reactionary, conservative`. The more expansive and accurate you can make your target set, the better the system will work. "]},{"cell_type":"markdown","metadata":{"id":"riINX9wVyFRJ","colab_type":"text"},"source":["## Existing target sets - details\n","\n","*Gender target sets taken from Nosek, Banaji, and Greenwald 2002.*\n","\n","Female: `sister, female, woman, girl, daughter, she, hers, her`.\n","\n","Male: `brother, male, man, boy, son, he, his, him`.\n","\n","\n","*Religion target sets taken from Garg et al. 2018.*\n","\n","Islam: `allah, ramadan, turban, emir, salaam, sunni, koran, imam, sultan, prophet, veil, ayatollah, shiite, mosque, islam, sheik, muslim, muhammad`.\n","\n","Christianity: `baptism, messiah, catholicism, resurrection, christianity, salva-tion, protestant, gospel, trinity, jesus, christ, christian, cross,catholic, church`.\n","\n","*Racial target sets taken from Garg et al. 2017*\n","\n","White last names: `harris, nelson, robinson, thompson, moore, wright, anderson, clark, jackson, taylor, scott, davis, allen, adams, lewis, williams, jones, wilson, martin, johnson`.\n","\n","Hispanic last names: `ruiz, alvarez, vargas, castillo, gomez, soto,gonzalez, sanchez, rivera, mendoza, martinez, torres, ro-driguez, perez, lopez, medina, diaz, garcia, castro, cruz`.\n","\n","Asian last names: `cho, wong, tang, huang, chu, chung, ng,wu, liu, chen, lin, yang, kim, chang, shah, wang, li, khan,singh, hong`.\n","\n","Russian last names: `gurin, minsky, sokolov, markov, maslow, novikoff, mishkin, smirnov, orloff, ivanov, sokoloff, davidoff, savin, romanoff, babinski, sorokin, levin, pavlov, rodin, agin`.\n","\n","\n","*Career/family target and attribute sets taken from Garg et al. 2018.*\n","\n","Career: `executive, management, professional, corporation, salary, office, business, career`.\n","\n","Family: `home, parents, children, family, cousins, marriage, wedding, relatives.Math: math, algebra, geometry, calculus, equations, computation, numbers, addition`.\n","\n","\n","*Arts/Science target and attribute sets taken from Garg et al. 2018.*\n","\n","Arts: `poetry, art, sculpture, dance, literature, novel, symphony, drama`.\n","\n","Science: `science, technology, physics, chemistry, Einstein, NASA, experiment, astronomy`."]},{"cell_type":"markdown","metadata":{"id":"aHCHMAPYTCVX","colab_type":"text"},"source":["### Sources\n","\n","Nosek, B. A., Banaji, M. R., & Greenwald, A. G. (2002). Harvesting implicit group attitudes and beliefs from a demonstration web site. Group Dynamics, 6(1), 101–115. https://doi.org/10.1037/1089-2699.6.1.101\n","\n","Garg, N., Schiebinger, L., Jurafsky, D., & Zou, J. (2017). Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes, 1–33."]}]}