{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Week 1-3 exercises - INTERMEDIATE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5YUirSNPmwXf"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WClw_VPOmwXd",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"display: block; width: 100%; height: 120px;\">\n",
        "\n",
        "<p style=\"float: left;\">\n",
        "    <span style=\"font-weight: bold; line-height: 24px; font-size: 16px;\">\n",
        "        DIGHUM160 - Critical Digital Humanities\n",
        "        <br />\n",
        "        Digital Hermeneutics 2019\n",
        "    </span>\n",
        "    <br >\n",
        "    <span style=\"line-height: 22x; font-size: 14x; margin-top: 10px;\">\n",
        "        Created by Tom van Nuenen (tom.van_nuenen@kcl.ac.uk) <br />\n",
        "    </span>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36-_8ZvxmwXf",
        "colab_type": "text"
      },
      "source": [
        "# Welcome!\n",
        "\n",
        "In this Notebook we will go over some of the things you have learned in the past month. \n",
        "Feel free to collaborate with each other in order to answer these questions. And don't forget, Stack Overflow is your friend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YUirSNPmwXf",
        "colab_type": "text"
      },
      "source": [
        "## 1. Working with .txt files\n",
        "\n",
        "Write a small utility function `read_file(filename)` that reads a specified file and simply returns all contents as a single string. Use it to read the example file 'test.txt' and save it in the variable `test`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuiuknq0cMe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6Ih4fo9gax5",
        "colab_type": "text"
      },
      "source": [
        "Next, you need to make sure you have the test.txt file in your Google Drive!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr8bu07_cOQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"14I5tYu0RDpa3k5YMJ1oJ4tXZJGBsSsGV\"})   \n",
        "downloaded.GetContentFile('test.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRJWdiHZccli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i6-ZX6mmwXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file(filename):\n",
        "    \"Read the contents of FILENAME and return as a string.\"\n",
        "    # insert your code here\n",
        "\n",
        "    \n",
        "    \n",
        "# this should work if your code is correct\n",
        "test = read_file('test.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT6CadvjmwXl",
        "colab_type": "text"
      },
      "source": [
        "Now, we are going to create a function `split_sentences` that performs some very simple sentence splitting when passed a text string. Each sentence will be represented as a new string, so the function as a whole returns a list of sentence strings. We assume that any occurrence of either  . or ! or ? marks the end of a sentence.\n",
        "\n",
        "First, you'll create a function called `end_of_sentence_marker` that takes as argument a character and returns True if it is an end-of-sentence marker, otherwise it returns False."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLFgg9QTmwXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define your function here\n",
        "def end_of_sentence_marker(character):\n",
        "    # insert your code here\n",
        "\n",
        "    \n",
        "\n",
        "# these tests should return True if your code is correct\n",
        "print(end_of_sentence_marker(\"?\") == True)\n",
        "print(end_of_sentence_marker(\"a\") == False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BemPS6vLmwXo",
        "colab_type": "text"
      },
      "source": [
        "An important function we will use is the built in `enumerate`. `enumerate` takes as argument any iterable (a string a list etc.). Let's see it in action:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOl58CxjmwXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, character in enumerate(\"Python\"):\n",
        "    print(i, character)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onXIjY0SmwXs",
        "colab_type": "text"
      },
      "source": [
        "As you can see, enumerate allows you to iterate over an iterable and for each element in that iterable, it gives you its corresponding index. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyEfG_65mwXs",
        "colab_type": "text"
      },
      "source": [
        "Now we can create our function `split_sentences`. See if you understand how it works!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqcaAMPmmwXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_sentences(text):\n",
        "    \"Split a text string into a list of sentences.\"\n",
        "    sentences = []\n",
        "    start = 0\n",
        "    for end, character in enumerate(text):\n",
        "        if end_of_sentence_marker(character):\n",
        "            sentence = text[start: end + 1]\n",
        "            sentences.append(sentence)\n",
        "            start = end + 1\n",
        "    return sentences\n",
        "\n",
        "split = split_sentences(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ggUeYWSmwXw",
        "colab_type": "text"
      },
      "source": [
        "Within `split_sentences`, we define a variable 'sentences' in which we store the individual sentences. Next, we define a variable `start` and set it to zero. We're doing this as we need to extract both the start position and the end position of each sentence, and we know that the first sentence will always start at position 0.\n",
        "\n",
        "Next, we use `enumerate` to *loop* over all individual characters in the text. Remember that enumerate returns pairs of indexes and their corresponding elements (here characters). For each character we check whether it is an end-of-sentence marker. If it is, the variable end marks the position in text where a sentence ends. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVXkXvx8mwXw",
        "colab_type": "text"
      },
      "source": [
        "There is an easier way to do this, however, which is through NLTK. Look up the `sent_tokenize` package, import it, and run it on our `test` data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObxTV_7_mwXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QklXaMOgmwXz",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's visualize some of these results. We'll create a new variable called `sentence_length`, assigning an empty list to it. We'll then loop over our `split` variable (which contains all split sentences in our test file) and add the length of each sentence to the `sentence_length` variable (tip: use the built-in `len()` function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ForANRI1mwX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBdsaP2RmwX3",
        "colab_type": "text"
      },
      "source": [
        "Finally, we'll import matplotlib and plot `sentence_length`. If you did everything right, the below code should give you a graph of the sentence lengths!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAxAJwvimwX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(sentence_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgBDYstgmwX6",
        "colab_type": "text"
      },
      "source": [
        "## 2. Working with Pandas\n",
        "\n",
        "Next, we'll import a .csv into a Pandas dataframe. But first we have to get it. One way to do this is by using PyDrive. We need to authenticate ourselves to Google so we can access a unique file ID on our Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnrwoPhVEs_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACZjMnD0E3B3",
        "colab_type": "text"
      },
      "source": [
        "Next, locate the file using its ID (to find the ID, go to your Drive folder, right-click on the file you want the ID of, and click \"Get shareable link\". The ID can be found in the link the Google gives you. For instance, in this link:\n",
        "\n",
        "```\n",
        "https://drive.google.com/file/d/1v2SYkCtIX6Pwg7rZyb25-Z2olLI8eNf_/view?usp=sharing\n",
        "```\n",
        "\n",
        "The ID is everything between the two forward brackets after `d/` and before `/view`. So: `1v2SYkCtIX6Pwg7rZyb25-Z2olLI8eNf_`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BFgs1xhj1V9o",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1v2SYkCtIX6Pwg7rZyb25-Z2olLI8eNf_\"})   \n",
        "downloaded.GetContentFile('AskTrumpSupporters.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc4PDVYGBjie",
        "colab_type": "text"
      },
      "source": [
        "Alternatively, you can use the `drive' method to \"mount\" your Google Drive within this notebook (meaning you can access all your files from there)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7Cd_Ze8BfIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKckZ1DREBmJ",
        "colab_type": "text"
      },
      "source": [
        "We can now navigate within our Google Drive folders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWrHsWO2Bhii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "os.chdir(\"/content/drive/My Drive/DIGHUM160-resources\") # You may need to change this to point to your location of the \"DIGHUM160-resources\" folder\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfgUmePwDb87",
        "colab_type": "text"
      },
      "source": [
        "You can also use the Drive API to up- and download files to your Drive from a notebook, and so on. See https://colab.research.google.com/notebooks/io.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzhPGc5HEe7x",
        "colab_type": "text"
      },
      "source": [
        "Next, we will import Pandas and use the read_csv function to open the example file 'AskTrumpSupporters.csv'. \n",
        "Then, show the first few lines of the dataframe (using the `.head()` function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkI-FuY2mwX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"AskTrumpSupporters.csv\") \n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMTVlnAmwYB",
        "colab_type": "text"
      },
      "source": [
        "Now, write a function `tokenizer()` that takes as input a string. Make sure that the function:\n",
        "- turns the string into lower case \n",
        "- cleans up punctuation (you can use regex, a custom list, or the inbuilt `string.punctuation` package)\n",
        "- puts al words in a list\n",
        "- returns said list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDP2cOAk42wN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk \n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YGvN2GhmwYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "def tokenizer(text):\n",
        "    '''cleans up and tokenizes input string'''\n",
        "    text = text.lower()\n",
        "    bad_chars = ['\\n', '\\t', '”', '“']\n",
        "    textClean = text.translate(str.maketrans({ch: \" \" for ch in bad_chars}))\n",
        "    table = str.maketrans({ch: None for ch in string.punctuation})\n",
        "    no_punct = (s.translate(table) for s in textClean.split(' ') if s != '')\n",
        "    digits_out = [word for word in no_punct if not word.isdigit()]\n",
        "    return digits_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZOwLYOAkWLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvtXxBUnmwYE",
        "colab_type": "text"
      },
      "source": [
        "Next, we might want to use our tokenizer on only a subset of our data. Create a new dataframe which only contains the first 20 entries of our df. Also, remove all rows containing empty values (you can use Pandas' `dropna()` method)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Tp6sayPmwYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh1Ro9rVAaCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrK94GYTmwYH",
        "colab_type": "text"
      },
      "source": [
        "Now let's run our tokenizer function, taking as input the `body` column from your dataframe. Loop over each row of your dataframe, and print out the tokenized `body` of each row to see if it works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TApMXYHAMnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a2w-4BvmwYJ",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll create the type-token ratio for each user in our df, to see whose language is the most 'complex'. First, we'll create a function for you that computes the TTR (see if you understand how it works!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lo1Gz5hmwYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def typeTokenRatio(tokens): \n",
        "    numTokens = len(tokens)\n",
        "    numTypes = len(set(tokens))\n",
        "    return numTypes/numTokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPddUWELmwYM",
        "colab_type": "text"
      },
      "source": [
        "Finally, loop over the 'body' column of each row in your df again. This time, within the loop, create a variable `tokens` and assign to it the output of your tokenizer function. Then, print the output of the `typeTokenRatio` function, which you run on `tokens`.\n",
        "\n",
        "If things go well, you'll see the TTR for each of the 20 posts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xghYJeQ1mwYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znmgu5ncmwYO",
        "colab_type": "text"
      },
      "source": [
        "We see that some posts have a TTR of 1, meaning all words are unique. In fact, TTR does not tell us much here, as these are all short posts. But anyway: great work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdMGp8AomwYP",
        "colab_type": "text"
      },
      "source": [
        "## BONUS ROUND\n",
        "\n",
        "If you still have time, try to write a short program that tells you the *10 most-used words* for a given user comment in the DataFrame. Tip: use the `Counter` class from the `collections` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaA19lVKmwYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}