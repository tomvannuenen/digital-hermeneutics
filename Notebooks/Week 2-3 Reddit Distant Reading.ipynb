{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Week 2-3 Reddit Distant Reading.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K5Zn8qobjic",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"display: block; width: 100%; height: 120px;\">\n",
        "\n",
        "<p style=\"float: left;\">\n",
        "    <span style=\"font-weight: bold; line-height: 24px; font-size: 16px;\">\n",
        "        DIGHUM160 - Critical Digital Humanities\n",
        "        <br />\n",
        "        Digital Hermeneutics 2020\n",
        "    </span>\n",
        "    <br >\n",
        "    <span style=\"line-height: 22x; font-size: 14x; margin-top: 10px;\">\n",
        "        Week 2-3: Distant reading on Reddit <br />\n",
        "        Created by Tom van Nuenen (tom.van_nuenen@kcl.ac.uk)\n",
        "    </span>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJHDHrQPbjif",
        "colab_type": "text"
      },
      "source": [
        "# Distant Reading on Reddit\n",
        "\n",
        "This notebook focuses on some basics to Pandas, as well as some methods to engage in a simple distant reading using NLTK. By the end of this notebook, you will:\n",
        "\n",
        "* Know how to open and perform simple operations on a DataFrame;\n",
        "* Use NLTK's `Text()` object to perform some basic distant reading operations on a subreddit.\n",
        "\n",
        "We'll be using data from the subreddit [r/seduction](https://via.hypothes.is/https://www.reddit.com/r/seduction/top/?t=all). The community describes itself as a space for \"Help with dating, with a focus on how to get something started up, whether the goal is casual sex or a relationship. Learn how to connect with the ones you're trying to get with!\"\n",
        "\n",
        "Begin your investigation by taking 10 minutes to explore [some of the most popular posts](https://via.hypothes.is/https://www.reddit.com/r/seduction/top/?t=all) using hypothes.is. The dataset we'll be using only includes posts, so you can disregard the comments for now. \n",
        "\n",
        "When you have identified some themes, return back here to take a more \"distant\" perspective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLTXX5Dtbjii",
        "colab_type": "text"
      },
      "source": [
        "## Importing packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBB4BWsPbjij",
        "colab_type": "text"
      },
      "source": [
        "Let's start by importing some packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw1SGwFsbjik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.text import Text\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import collections\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6erwYIhh-cR",
        "colab_type": "text"
      },
      "source": [
        "## Getting Reddit data\n",
        "Let's authenticate ourselves and get the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjxJwncSo4_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYUfVU8xp3eN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1fOe3l9dLKb51jrwqUNOvwO4A7F7sM6Xx\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('seduction-submissions.csv')        # replace the file name with your file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "OFKIEs96ZFW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sed = pd.read_csv(\"seduction-submissions.csv\", lineterminator=\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuIFFo8clcwo",
        "colab_type": "text"
      },
      "source": [
        "## Pandas basics & Working with Reddit data\n",
        "\n",
        "Using the `.head()` method we can get the first n rows of a df. The default is 5. We can add a *parameter* (here 3) to indicate how many rows we want to print."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zipTYx8rlbOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sed.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9za2bgknrbRx",
        "colab_type": "text"
      },
      "source": [
        "Here's what we're seeing. Pay special attention to the \"NaN\" labels, indicating missing values (we might want to get rid of them). Also remember the naming convention for the column and row axes (which Pandas uses when accessing particular rows/columns).\n",
        "![df](http://www.digitalhermeneutics.com/wp-content/uploads/2020/07/df.png)\n",
        "\n",
        "This particular dataset only includes the original posts in the subreddit (so not the comments on the posts). The \"selftext\" column contains the actual posts.\n",
        "\n",
        "other columns contain valuable metadata you can use in your analyses, such as: \n",
        "- \"created\" (the time of the post's creation)\n",
        "- \"score\" (amount of upvotes minus downvotes)\n",
        "- \"textlen\" (amount of words)\n",
        "- \"num_comments\" (the amount of comments)\n",
        "- \"flair_text\" (a 'tag' that users within a subreddit can add)\n",
        "- \"augmented_count\" (how often a user or moderator has edited the text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlu74LCjjNhm",
        "colab_type": "text"
      },
      "source": [
        "### Sorting a DF\n",
        "Using the `.sort_values()` method we can sort the df by particular columms. We use two parameters: the `by` parameter indicates by which column we want to sort, the `ascending` parameter indicated whether our sortation is in ascending or descending order.\n",
        "\n",
        "Here, I'm assigning my sorted DataFrame to the same variable `sed`, effectiveluy overwriting the old version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcpxg8ezjNxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sed = sed.sort_values(by=['score'], ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK2VNDu71Wss",
        "colab_type": "text"
      },
      "source": [
        "Your turn! Sort the DataFrame by **creation date** (look up the name of this column first), and set `ascending` to `True`. Then, assign that sorted dataframe to the same variable name, `sed`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NKlr_rn1VWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ux_P621yEM8",
        "colab_type": "text"
      },
      "source": [
        "### Converting to datetime\n",
        "Did you ever wonder which format the \"created\" column is in? It is a Unix timestamp: the number of seconds that have elapsed since the Unix epoch, minus leap seconds; the Unix epoch is 00:00:00 UTC on 1 January 1970."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY9HSe-O1HSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.to_datetime(1207632114,unit='s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Poupzh82zGt",
        "colab_type": "text"
      },
      "source": [
        "Pandas allows us to create a new column evaluating the Unix timestamp to more readable datetimes using the `.to_datetime` method. \n",
        "\n",
        "Creating a new column in Pandas is as easy as using the bracket notation to write a new column name, then assigning it. In this case, we just use the `.to_datetime` method again to point to the entire \"created\" column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6O7Y8cSzkmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sed['created_datetime'] = pd.to_datetime(sed['created'],unit='s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9-L4LR4lDlz",
        "colab_type": "text"
      },
      "source": [
        "### Selecting a column\n",
        "To select a single column of data, simply put the name of the column in between brackets. Let’s select the 'selftext' column. We can print out the first entry in this column as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hmKORvYlFN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sed['selftext'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ppsb8JK9tt4j",
        "colab_type": "text"
      },
      "source": [
        "As you see, using the `[]` operator selects a set of rows and/or columns from a DataFrame.\n",
        "\n",
        "Your turn! Use slicing to retrieve the first 10 \"selftext\" entries in oue DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjxlNNK_syRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWrEqa9rvL8U",
        "colab_type": "text"
      },
      "source": [
        "One thing we often do when we’re exploring a dataset is filtering the data based on a given condition. For example, we might need to find all the rows in our dataset where the score is over 500. We can use the `.loc[]` method to do so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrlTcUzKvK_Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sed.loc[sed.score >= 500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym6Q_twP5E2y",
        "colab_type": "text"
      },
      "source": [
        "`.loc[]` is a powerful method that can be used for all kinds of research purposes, if you want to filter or prune your dataset based on some condition. For more info, see [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMglX5TU0J5F",
        "colab_type": "text"
      },
      "source": [
        "Your turn! Use `loc[]` to retrieve only the posts that have over 200 comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HX3cHuN0Jd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSdM51N2bjkc",
        "colab_type": "text"
      },
      "source": [
        "## Distant reading with NLTK \n",
        "Tomorrow, we will look at preprocessing our text in more detail. For now, let's automate most of it using NLTK's `word_tokenize()` method. We've imported this library at the beginning of this notebook.\n",
        "`word_tokenize()` works like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzuuVD23-QNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_tokenize(\"He is a lumberjack and he is okay. He sleeps all night and he works all day.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OscceKEtbjkd",
        "colab_type": "text"
      },
      "source": [
        "Your turn! Let's tokenize our \"selftext\" column. Here's what you need to do: \n",
        "- Create a new list called `sed_tokens`;\n",
        "- Begin a for-loop that iterates over the \"selftext\" column of our `sed` DataFrame; \n",
        "- `For` each text in that column, tokenize it using `word_tokenize()`; \n",
        "- Add these tokenized words to our new `sed_tokens` list using the list `.extend()` method*.\n",
        " \n",
        "*We use `.extend` instead of `.append`. This is because we want one long list, instead of a list of lists. While `append` adds its argument as a single element to the end of a list – meaning the length of the list itself will increase by one – `extend` adds each element to the list, extending the list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqrTrJ33bjkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK1DCuAJbjkh",
        "colab_type": "text"
      },
      "source": [
        "## The NLTK `Text()` class\n",
        "Now, let's have a look at our data. NLTK provides a `Text()` class, which is a \"wrapper\" that allows for inital exploration of texts. It supports counting, concordancing, collocation discovery, etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58NEJVb2bjki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sed_t = Text(sed_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAdI-HgMbjkl",
        "colab_type": "text"
      },
      "source": [
        "Let's print out the \"docstring\" of NLTK's `Text()` object, as well as all the things you can do with this object. Have a read through this to see what it allows you to do!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "GvexG4e7bjkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "help(Text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPatGDHDOpaU",
        "colab_type": "text"
      },
      "source": [
        "### Concordances \n",
        "One of the most basic, but quite helpful, ways to quickly get an overview of the contexts in which a word appears is through a concordance view. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkDsXJkPbjkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sed_t.concordance('game', width=115)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np8M59BeDJll",
        "colab_type": "text"
      },
      "source": [
        "### Word plotting\n",
        "Using the `dispersion_plot()` method we can easily visualize how often some word appears throughout the text. We have to feed it a list with several words in it.\n",
        "\n",
        "Sorting our df by date allows us to look \"through time\" to see whether particular words start (dis)appearing in our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_myUpUcqDIdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sed_t.dispersion_plot([\"feminism\", \"feminist\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5q_G1GKkZSy",
        "colab_type": "text"
      },
      "source": [
        "### Similar words\n",
        "Using the `.similar()` method we can look at \"distributional similarity\": finding other words which appear in the same contexts as the specified word.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWhoVtkrjBx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sed_t.similar('love')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoVGlkEWbjks",
        "colab_type": "text"
      },
      "source": [
        "**Exploring texts using NLTK**\n",
        "\n",
        "Use these NLTK methods on this dataset. The `Text()` object has other functionalitities; look through the `help(Text)` instructions we just printed out if you want to check them out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxqDTbaYbjkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}