{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: block; width: 100%; height: 120px;\">\n",
    "\n",
    "<p style=\"float: left;\">\n",
    "    <span style=\"font-weight: bold; line-height: 24px; font-size: 16px;\">\n",
    "        DIGHUM160 - Critical Digital Humanities\n",
    "        <br />\n",
    "        Digital Hermeneutics 2019\n",
    "    </span>\n",
    "    <br >\n",
    "    <span style=\"line-height: 22x; font-size: 14x; margin-top: 10px;\">\n",
    "        Week 2 exercises: CLOSE AND DISTANT READING <br />\n",
    "        Created by Tom van Nuenen (tom.van_nuenen@kcl.ac.uk)\n",
    "    </span>\n",
    "</p>\n",
    "\n",
    "<img style=\"width: 240px; height: 120px; float: right; margin: 0 0 0 0;\" src=\"http://www.merritt.edu/wp/histotech/wp-content/uploads/sites/275/2018/08/berkeley-logo.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distant Reading\n",
    "\n",
    "This notebook focuses on some simple methods for close and distant reading using NLTK. Make sure you have this package installed (if you have Acaconda, it should be there)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this notebook, you should:\n",
    "\n",
    "* have practiced with moving from close to distant reading, and applying research questions using Natural Language Processing methods;\n",
    "* be able to work with some basic natural language processing methods using NLTK.\n",
    "\n",
    "**Make sure to read and respond to the assignment at the end of the notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distant reading Ferrante and Knausgård\n",
    "\n",
    "For this distant reading, you can explore the themes you found in your close reading related to how Knausgard and Ferrante (either separately, or in relation to the other) construct gendered and bodily identity. \n",
    "\n",
    "If you have found a pattern you want to explore yourself, you’re free to do so. Otherwise, read the next paragraph for a more pointed question. \n",
    "\n",
    "_Film scholar Linda Williams (1991) has written about the so-called ‘body genres’: pornography, horror, and melodrama that are characterized by excess that cause these genres to be often classified as ‘low culture.’ This is related to the fact that such works of art elicit certain bodily reactions in the viewer, framed in specifically gendered ways (‘tear jerker,’ ‘fear jerker,’ to ‘jerk off’) that make them suspect because of a lack of proper aesthetic distance and a sense of over-involvement in sensation and emotion. We can apply Williams’ notions to the literary texts of Ferrante and Knausgård to see what this yields in terms of gender analysis, by mining words related to bodily materiality, such bodily fluids: e.g. \"sweat\" or \"tears\". This can then be offset against cerebral terms, e.g. \"spirit\" or \"thinking\"._ \n",
    "\n",
    "Try to think about how you could operationalize the patterns and tensions we discussed yesterday by looking for **keywords, word contexts, and stylistic differences** between these two authors. \n",
    "\n",
    "* First, make a list of as many possible English words related to the topics you have encountered in your close reading. Make sure to include disambiguations. If you are interested in ‘tears,’ for instance, also include ‘cry,’ ‘cries,’ ‘crying’ etc.  \n",
    "\n",
    "* Explore the different ways, described below, in which distant reading might help you better understand these themes and topics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.text import Text\n",
    "\n",
    "import collections\n",
    "import string\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get our data. Locate the data files in the folder this notebook is in (they should be in the same folder if you want to access them!). Look closely at what they're called. Now we read them into python by calling `open()` and `read()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that ferrante.txt and knaus.txt are in the same folder as this notebook!\n",
    "ferrante = open('ferrante.txt', encoding=\"UTF-8\").read().lower()\n",
    "knaus = open('knaus.txt', encoding=\"UTF-8\").read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we got our data. Now, we need to create a tokenizer. A tokenizer is basically a function that takes a string and outputs the words into seperate entities (like a list). You can create your own, but NLTK helpfully has one too. \n",
    "Let's see the NLTK's `word_tokenize()` function in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferranteTokens = word_tokenize(ferrante)\n",
    "knausTokens = word_tokenize(knaus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if it works!\n",
    "knausTokens[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how long our lists are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ferranteTokens))\n",
    "print(len(knausTokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out Knausgard's text is much longer than Ferrante's (almost double!) This means we'll have to normalize many of the findings we make. \n",
    "**Think about why this is important.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizers are great, but they're often not perfect. Look at the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_tokenize(\"Why won't this work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, looks like it did a pretty good job, except it considers \"wo\" and \"n't\" as different words.. Annoying. This is where **stemming** and **lemmatizing** come in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to load our stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for each in [\"think\", \"thinker\", \"thinking\"]:\n",
    "    print(stemmer.stem(each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but stemming doesn't always produce the prettiest results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in [\"create\", \"creating\", \"creator\"]:\n",
    "    print(stemmer.stem(each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "A lemma is the canonical, dictionary or citation form of a word. For instance, the lemma for \"thinks\" is \"think.\" \n",
    "Lemmas typically are a bit less intrusive than stemmers to your data. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in [\"trade\", \"trades\", \"trading\", \"trader\", \"traders\"]:\n",
    "    print(lemmatizer.lemmatize(each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional challenge: lemmatizing / stemming**\n",
    "\n",
    "*Note: if the below exercise is too difficult, don't worry about it for now--just tokenize your data using `word_tokenizer()`*.\n",
    "\n",
    "We're going to tokenize our data, then stem or lemmatize it. In order to do that, you have to:\n",
    "1. Assign your tokenized data to a variable\n",
    "2. Create a new empty list, assigning it to a variable\n",
    "3. Create a `for`-loop that iterates through all the words in your tokenized data\n",
    "4. For each word you loop through, assign that word to some variable\n",
    "5. ...and add the word to your new list\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequencies and Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK allows us to analyze the word frequencies for both Ferrante and Knausgard. First, we should remove punctuation from our text, tokenize them, remove stopwords, and lowercase each token. In the future, we can write a long function to do all of this at once, but for now, let's see how it works step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we define a function that will strip punctuation from a string (your `knaus` and `ferrante` variables)\n",
    "def stripPunctuation(s):\n",
    "    return ''.join(ch for ch in s if ch not in string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run this function on our two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferranteNoPunct = stripPunctuation(ferrante)\n",
    "knausNoPunct = stripPunctuation(knaus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferranteTokens = word_tokenize(ferranteNoPunct)\n",
    "knausTokens = word_tokenize(knausNoPunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripStopwords(tokens):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    return [w for w in tokens if not w in stopWords] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferranteTokensClean = stripStopwords(ferranteTokens)\n",
    "knausTokensClean = stripStopwords(knausTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if it worked!\n",
    "knausTokensClean[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start counting these words. Using the `Counter()` function (from the `collections` module), we can count the elements in our lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferranteCounts = collections.Counter(ferranteTokensClean)\n",
    "knausCounts = collections.Counter(knausTokensClean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this to count any individual word type in both our texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knausCounts['heart']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we were talking about normalizing our frequencies? That's what we'll do next. We'll build a dictionary to compare the relative word proportions in Ferrante's and Knausgard's texts. This way, we can define the **keywords** for both authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knausKeys = {}\n",
    "for word in knausCounts: \n",
    "    # How often does Knausgard use the word we're interested in? \n",
    "    knausCount = knausCounts[word]\n",
    "    # Normalizing the frequencies\n",
    "    knausProportion = knausCount / len(knausTokens)\n",
    "    # Now we need to compare this to Ferrante's use of the same word. \n",
    "    # We will use the dictionary `.get()` method, as it allows us to return something even if the word\n",
    "    # isn't in our dictionary.\n",
    "    ferranteCount = ferranteCounts.get(word, 0)\n",
    "    ferranteProportion = ferranteCount / len(ferranteTokens)  \n",
    "    # We can now define the \"keywords\" for Knausgard, which is the relative proportion of this word \n",
    "    # as compared to Ferrante.\n",
    "    knausKey = (knausProportion - ferranteProportion)*100\n",
    "    # Finally, we add the word to our dictionary\n",
    "    knausKeys[word] = knausKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's retrieve some key-value pairs from our new dictionary \n",
    "first2pairs = {k: knausKeys[k] for k in list(knausKeys)[:7]}\n",
    "first2pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can find the top 10 words for Knausgard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knausSorted = sorted(knausKeys.items(), key=lambda item: item[1], reverse=True)\n",
    "for key, value in knausSorted[:10]: \n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A concordance list is an alphabetical list of the words (especially the important ones) present in a text, usually with citations of the passages concerned. Using NLTK, we can fairly easily create a concordance list of our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get our punctuation back, as it reads a bit easier\n",
    "ferranteTokens = word_tokenize(ferrante)\n",
    "knausTokens = word_tokenize(knaus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK contains a `Text()` object, which is a \"wrapper\" that supports inital exploration of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we create our NLTK Text object\n",
    "ferranteT = Text(ferranteTokens)\n",
    "knausT = Text(knausTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out thze \"docstring\" of NLTK's `Text()` object, as well as all the things you can do with this object. **Have a read through this** to see what it allows you to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "help(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knausT.concordance('heart', width=115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional challenge: exploring texts using NLTK**\n",
    "\n",
    "If the above is familiar, and you have explored the data using these methods, you are invited to use other NLTK methods on our texts. The `Text()` object has many other functionalitities, for instance. You can also have a look at POS tagging: https://www.nltk.org/book/ch05.html. Use it to seek out specific nouns or verbs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSIGNMENT\n",
    "\n",
    "1.\tWhat functionalities of NLTK did you use, and what patterns were you able to find?\n",
    "2.\tHow did these methods help you answer the questions you had?\n",
    "3.\tWhat have you not been able to answer using these methods?\n",
    "4.\tHow did the move from close to distant reading influence your findings?\n",
    "\n",
    "**Respond to these questions in the following Google Docs (don't forget to include your name):**\n",
    "https://docs.google.com/document/d/1WxElsUWg8WHQB_wh8N_35maDgwXpIP6Gi6HyVUpffMk/edit?usp=sharing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
