{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: block; width: 100%; height: 120px;\">\n",
    "\n",
    "<p style=\"float: left;\">\n",
    "    <span style=\"font-weight: bold; line-height: 24px; font-size: 16px;\">\n",
    "        DIGHUM160 - Critical Digital Humanities\n",
    "        <br />\n",
    "        Digital Hermeneutics 2019\n",
    "    </span>\n",
    "    <br >\n",
    "    <span style=\"line-height: 22x; font-size: 14x; margin-top: 10px;\">\n",
    "        Week 5-3: EVALUATING TOPIC MODELS<br />\n",
    "        Created by Tom van Nuenen (tom.van_nuenen@kcl.ac.uk)\n",
    "    </span>\n",
    "</p>\n",
    "\n",
    "<img style=\"width: 240px; height: 120px; float: right; margin: 0 0 0 0;\" src=\"http://www.merritt.edu/wp/histotech/wp-content/uploads/sites/275/2018/08/berkeley-logo.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating topic models\n",
    "\n",
    "Today, we'll try to improve our topic modeling. We'll use the `Gensim` package to create our topic models, as it allows us to run tests to optimize our topic amount.\n",
    "\n",
    "After reading this notebook, you'll be able to:\n",
    "\n",
    "1. Use gensim (including the MALLET wrapper) to create topic models;\n",
    "2. Evaluate topic models using several methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import string\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import re \n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Logging if you want it\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# Suppressing it should you want to\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "# Suppressing warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating submissions and comments\n",
    "\n",
    "For the topic model we're going to make, let's concatenate the associated submissions and comments (i.e., threads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load submissions and comments into DFs\n",
    "trpS = pd.read_csv(\"TRP-submissions-full.csv\", lineterminator='\\n')\n",
    "trpS = trpS.drop(['augmented_at', 'augmented_count', 'distinguish'], axis=1)\n",
    "trpS = trpS[~trpS['selftext'].isin(['[removed]', '[deleted]' ])].dropna(subset=['selftext'])\n",
    "\n",
    "trpC = pd.read_csv(\"TRP-comments-full.csv\", lineterminator='\\n')\n",
    "trpC = trpC[~trpC['body'].isin(['[removed]', '[deleted]' ])].dropna(subset=['body'])"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do an inner join of our two DataFrames. This will yield a new DF which only contains those submissions that have associated comments (based on their \"idstr\" and \"parent\" values).\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge DF based on idstr and parent\n",
    "trpT = pd.merge(trpS, trpC, how='inner', left_on='idstr', right_on='parent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trpT.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's iterate over our new DF and group all associated submissions and comments together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataD = {}\n",
    "for i, r in trpT.iterrows():\n",
    "    if r.idstr_x not in dataD.keys():\n",
    "        dataD[r.idstr_x] = [r.selftext, r.body]    \n",
    "    else:\n",
    "        dataD[r.idstr_x].append(r.body)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if it works\n",
    "dataD['t3_6sbx6i']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll join the items in each of the values in our `dict` and put that in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['\\n'.join(thread) for thread in dataD.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get rid of the newlines and single quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove newline characters\n",
    "data = [re.sub('\\s+', ' ', txt) for txt in data]\n",
    "\n",
    "# Remove single quotes\n",
    "data = [re.sub(\"\\'\", \"\", txt) for txt in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Gensim's `simple_preprocess()` method this time. If you haven't seen `yield` before, it is used in what's called a generator function. This is simply a function that iterates, instead of only returning something once.\n",
    "\n",
    "`Return` sends a specified value back to its caller whereas `yield` can produce a sequence of values. We should use `yield` when we want to iterate over a sequence, but don’t want to store the entire sequence in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(texts):\n",
    "    for text in texts:\n",
    "        yield(gensim.utils.simple_preprocess(str(text), deacc=True))  # deacc=True removes punctuations\n",
    "tokens = list(tokenizer(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for each in tokens:\n",
    "    counter += len(each)\n",
    "counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exracting N-grams with gensim\n",
    "\n",
    "Topic modeling (as well as many other kinds of NLP methods) works better on N-grams, as it allows words that frequently appearing together to be concatenated (e.g. \"red pill\" means something different than \"red\" and \"pill\" separately).\n",
    "\n",
    "Gensim’s `Phrases` model implements bigrams, trigrams, quadgrams, etc. \n",
    "\n",
    "`Phrases` detects phrases based on collocation counts. It builds a model of input text that you then can use on other data.\n",
    "\n",
    "Gensim detects a bigram if a scoring function for two words exceeds a threshold. The two important arguments to `Phrases` are `min_count` and `threshold`. The higher the values of these parameters, the harder it is for words to be combined to bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build bigram and trigram models\n",
    "bigram = gensim.models.Phrases(tokens, min_count=5, threshold=100) # higher threshold = fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[tokens], threshold=100)  \n",
    "\n",
    "# `Phraser` must be built from an initial `Phrases` instance. \n",
    "# It then works a little faster while using much less memory. See https://radimrehurek.com/gensim/models/phrases.html\n",
    "bigramMod = gensim.models.phrases.Phraser(bigram)\n",
    "trigramMod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trigramMod[bigramMod[tokens[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some functions for stopword removal, making bigrams and trigrams, and lemmatization (using spacy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare stopwords\n",
    "stop = set(stopwords.words('english') + ['’', '“', '”', 'nbsp', 'http'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def removeStopwords(texts):\n",
    "    return [[word for word in doc if word not in stop] for doc in texts]\n",
    "\n",
    "def makeBigrams(texts):\n",
    "    return [bigramMod[doc] for doc in texts]\n",
    "\n",
    "def makeTrigrams(texts):\n",
    "    return [trigramMod[bigramMod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Remove stopwords\n",
    "tokensNoStops = removeStopwords(tokens)\n",
    "\n",
    "# Form trigrams\n",
    "trigrams = makeTrigrams(tokensNoStops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization \n",
    "lemmas = lemmatization(trigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this in a pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"TRP-nouns.text\", \"wb\") as docP: \n",
    "    pickle.dump(lemmas, docP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `Dictionary`\n",
    "\n",
    "Now, let's create our gensim dictionary - a mapping of each word to a unique id. \n",
    "It will be used to create a `Corpus` object, which is gensim’s equivalent of a Document-Term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary \n",
    "dictionary = corpora.Dictionary(lemmas)\n",
    "\n",
    "# Create Corpus, i.e. Document-Term Matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in lemmas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Gensim allows you to run the `.add_documents` method that will append documents to your `dictionary`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's view some of the corpus we have now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the first 10 bigrams above. Each consists of words with a unique id. This a mapping of (word_id, word_frequency). For example, (0, 1) above demonstrates that word id 0 occurs once in the first document. Word id 5 occurs 4 times, and so on. This is used as the input by the LDA model.\n",
    "\n",
    "If you want to see what word a given id corresponds to, pass the id as a key to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you want to see the associated id for some word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.token2id['pill']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save our dictionary into a gensim `corpora` object. We'll dump our BoW representation in a pickle as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corp.cor\", \"wb\") as fp: \n",
    "    pickle.dump(corpus, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should you want to load in the pickle we just saved:\n",
    "with open(\"corp.cor\", \"rb\") as cp: \n",
    "    corp = pickle.load(cp)\n",
    "\n",
    "# should you want to load in the dictionary we just saved:\n",
    "# dictionary = gensim.corpora.dictionary.Dictionary.load(\"Dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running an LDA model\n",
    "\n",
    "Now, let's run a Gensim LDA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build LDA model. Make sure to play around with chunksize and passes and check if coherence score changes a lot.\n",
    "\n",
    "ldaModel = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           # eval_every = 20, # this is the evaluation, perplexity\n",
    "                                           update_every=1,\n",
    "                                           chunksize=500,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save this model to HD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaModel.save(\"ldaModel-TRP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to load the finished LDA model from disk\n",
    "ldaModel = ldaModel.load(\"ldaModel-TRP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the model\n",
    "\n",
    "Let's try to evaluate our topics. First, we can visualize our topics using pyLDAvis. A \"good\" topic model produces non-overlapping, fairly large bubbles, which should be scattered throughout the chart instead of being clustered in one quadrant. A model with too many topics will typically have many overlaps, small sized bubbles clustered in one region of the chart. **This is the first way in which you can evaluate your topic models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's visualize our topics\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.gensim.prepare(ldaModel, corpus, dictionary)\n",
    "panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are we seeing?\n",
    "\n",
    "On the left, there is a 2D plot of the \"distance\" between all of the topics (labeled as the Intertopic Distance Map). This plot uses a multidimensional scaling (MDS) algorithm. \n",
    "- Similar topics should appear close together on the plot; dissimilar topics should appear far apart. \n",
    "- The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus.\n",
    "\n",
    "### Exploring topics and words\n",
    "- You can scrutinize a topic more closely by clicking on its circle, or entering its number in the \"selected topic\" box in the upper-left (Note that, though the data used by gensim and pyLDAvis are the same, they don't use the same ID numbers for topics.)\n",
    "- If you roll your mouse over a term in the bar chart on the right, the topic circles will resize in the plot on the left. This shows the strength of the relationship between the topics and the selected term.\n",
    "\n",
    "### Salience\n",
    "On the right, there is a bar chart with the top terms. When no topic is selected in the plot on the left, the bar chart shows the top-30 most **salient** terms in the corpus. A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics.\n",
    "\n",
    "### Probability Vs Exclusivity \n",
    "When you select a particular topic, this bar chart changes to show the top-30 most \"relevant\" terms for the selected topic. The relevance metric is controlled by the parameter λ, which can be adjusted with a slider above the bar chart:\n",
    "\n",
    "* Setting λ close to 1.0 (the default) will rank the terms according to their probability within the topic.\n",
    "* Setting λ close to 0.0 will rank the terms according to their \"distinctiveness\" or \"exclusivity\" within the topic. This means that terms that occur only in this topic, and do not occur in other topics.\n",
    "\n",
    "You can move the slider between 0.0 and 1.0 to weigh term probability and exclusivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Analyzing the pyLDA visualization\n",
    "\n",
    "The interactive visualization pyLDAvis produces is helpful for both **individual** topics: you can manually select each topic to view its top most frequent and/or \"relevant\" terms, using different values of the λ parameter. This can help when you're trying to assign a name or \"meaning\" to each topic. \n",
    "\n",
    "It also helps you to see the **relationships** between topics: exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics.\n",
    "\n",
    "See if you can make sense of the patterns you are seeing. Can you give an explanation for the distance between certain topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Topic Coherence and Perplexity\n",
    "\n",
    "Next, we can apply some statistical measures to help us determine the optimal number of topics in our topic model.\n",
    "\n",
    "**Topic Coherence** is a measure applied to the top N words from each topic. It is defined as the average / median of the pairwise word-similarity scores of the words in the topic. A good model will generate topics with *high* topic coherence scores. Good topics are topics that can be described by a short label, therefore this is what the topic coherence measure should capture.\n",
    "\n",
    "**Perplexity** is a measure of how well a probability model predicts a sample. As applied to LDA, for a given amount of topics (K), you estimate the LDA model. Then given the theoretical word distributions represented by the topics, compare that to the actual topic mixtures, or distribution of words in your documents. The benefit of this statistic comes in comparing perplexity across different models with varying K's. The model with the *lowest* perplexity is generally considered the “best”. *The issue with perplexity is that it tends to not be strongly correlated to human judgment and, even sometimes slightly anti-correlated. Therefore, we'll work with coherence scores*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute Perplexity\n",
    "#print('\\nPerplexity: ', ldaModel.log_perplexity(corpus))  # A measure of how good the model is. The lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherenceModel = CoherenceModel(model=ldaModel, corpus=corpus, texts=lemmas, dictionary=dictionary, coherence='c_v') \n",
    "# The higher the better. A coherence score of .4 means probably not right number of topics; .6 is great. Anything more is suspiciously great.\n",
    "coherence = coherenceModel.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no hard or fast rule on what makes a good coherence or perplexity score. We have to compare this for different iterations of our topic model (using different amounts of topics) to see which one works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MALLET\n",
    "\n",
    "One thing we can try first is using a different LDA implementation. MALLET sometimes yields beter results than Gensim. Gensim actually has a wrapper for MALLET, so let's see if it really is better.\n",
    "\n",
    "Download MALLET if you don't have it yet: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip. In the cell below, update the path to where your MALLET files are, relative to the path of this notebook (the code below assumes the unzipped MALLET folder is in the same folder as this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malletPath = 'mallet-2.0.8/bin/mallet' # update this path\n",
    "ldaMallet = gensim.models.wrappers.LdaMallet(malletPath, corpus=corpus, num_topics=20, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to show MALLET's topics\n",
    "# pprint(ldaMallet.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherenceModelLdaMallet = CoherenceModel(model=ldaMallet, corpus=corpus, texts=lemmas, dictionary=dictionary, coherence='c_v')\n",
    "coherenceLdaMallet = coherenceModelLdaMallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherenceLdaMallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That does seem a little better! But not much. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing coherence scores\n",
    "\n",
    "The most obvious thing we can do to find optimal scores is to play around with the amount of topics our model creates. One way to do this is to build many LDA models with different values of number of topics (k), and then pick the one that gives the highest coherence value. Choosing a ‘k’ at the end of a rapid growth of topic coherence usually yields meaningful and interpretable topics. If you see the same keywords being repeated in multiple topics, it’s probably a sign that the ‘k’ is too large.\n",
    "\n",
    "This `computeCoherenceValues()` function trains multiple LDA models, provides the models, and tells you their corresponding coherence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCoherenceValues(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherenceValues = []\n",
    "    modelList = []\n",
    "    totalAmount = limit / step\n",
    "    currentAmount = 0\n",
    "    for numTopics in range(start, limit, step):\n",
    "        #model = gensim.models.wrappers.LdaMallet(malletPath, corpus=corpus, num_topics=numTopics, id2word=dictionary)\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=numTopics, random_state=100, update_every=1, \n",
    "                                chunksize=500, passes=10, alpha='auto', per_word_topics=False)\n",
    "        modelList.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        # When using 'c_v' texts should be provided, corpus isn’t needed. \n",
    "        # When using ‘u_mass’ corpus should be provided, if texts is provided, it will be converted to corpus using the dictionary \n",
    "        coherenceValues.append(coherencemodel.get_coherence())\n",
    "        currentAmount += 1\n",
    "        print(\"Built \" + str(currentAmount) + \" of \" + str(totalAmount) + \" models\")\n",
    "\n",
    "    return modelList, coherenceValues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "modelList, coherenceValues = computeCoherenceValues(dictionary=dictionary, corpus=corpus, texts=lemmas, start=10, limit=100, step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=100; start=10; step=10;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherenceValues)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print these coherence scores\n",
    "for m, cv in zip(x, coherenceValues):\n",
    "    print(\" Num Topics =\", m, \"Coherence Value =\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the coherence score seems to keep increasing, it generally makes sense to pick the model that gave the highest CV before dropping again. We'll pick the most fitting amount of topics to continue our research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the ideal model and print the topics\n",
    "optimalModel = modelList[1]\n",
    "modelTopics = optimalModel.show_topics(formatted=False)\n",
    "pprint(optimalModel.print_topics(num_words=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling the topic model\n",
    "\n",
    "Let's explore the individual topics using a function. The function will ask for a label for each of the topics, which it will save to a `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(topicN, topn=25):\n",
    "    \"\"\"\n",
    "    Accept a user-supplied topic number and print out a formatted list of the top terms.\n",
    "    Allow use input to create dict for topic names\n",
    "    \"\"\"\n",
    "    topicDict = {}\n",
    "    for n in range(topicN):\n",
    "        print('TOPIC ' + str(n))\n",
    "        print('{:20} {}'.format('term', 'frequency') + u'\\n')\n",
    "        for term, frequency in ldaModel.show_topic(n, topn=25):\n",
    "            print('{:20} {:.3f}'.format(term, round(frequency, 3)))\n",
    "        topicDict[n] = input(\"Topic label\")\n",
    "        clear_output()\n",
    "    return topicDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topicDict = explore_topic(optimalModel.num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding most dominant topic per document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the practical application of topic modeling is to determine what topic a given document is about. To figure this out, we find the topic number that has the highest percentage contribution in that document. We'll write a `dominantTopic()` function that aggregates this information in a presentable table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dominantTopic(ldamodel=optimalModel, corpus=corpus, texts=lemmas):\n",
    "    # Create DF\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "df_topic_sents_keywords = dominantTopic(ldamodel=optimalModel, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding most distinctive document per topic\n",
    "\n",
    "We can also find the documents that include the highest amount of words for a certain topic. You could use this if you have found a really interesting topic, and you want to know what kinds of documents this topic is typically found in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(5)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet['Text'][25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic distribution across documents\n",
    "\n",
    "We can also look at the volume and distribution of topics in order to judge how widely each topic was discussed. The below table exposes that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional assignment: using the topic names\n",
    "\n",
    "If you want to dive a bit deeper into these functions, try to change the output so that it doesn't show the topic numbers, but the topic names from the `topicDict` we created!\n",
    "It will help you get a better understanding of what's going on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
