{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: block; width: 100%; height: 120px;\">\n",
    "\n",
    "<p style=\"float: left;\">\n",
    "    <span style=\"font-weight: bold; line-height: 24px; font-size: 16px;\">\n",
    "        DIGHUM160 - Critical Digital Humanities\n",
    "        <br />\n",
    "        Digital Hermeneutics 2019\n",
    "    </span>\n",
    "    <br >\n",
    "    <span style=\"line-height: 22x; font-size: 14x; margin-top: 10px;\">\n",
    "        Week 3-4: ACCESSING REDDIT API <br />\n",
    "        Created by Tom van Nuenen (tom.van_nuenen@kcl.ac.uk)\n",
    "    </span>\n",
    "</p>\n",
    "\n",
    "<img style=\"width: 240px; height: 120px; float: right; margin: 0 0 0 0;\" src=\"http://www.merritt.edu/wp/histotech/wp-content/uploads/sites/275/2018/08/berkeley-logo.jpg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the Reddit API\n",
    "\n",
    "The Reddit API allows you to do lots of things, such as automatically post as a user. It also allows you to retrieve data from Reddit, such as subreddit posts and comments. \n",
    "\n",
    "There are restrictions in place: Reddit's API only allows you to retrieve 1000 posts (and associated comments) per task. While we can create a script that takes note of the timecodes of posts so as to scrape the entiry of a subreddit in multiple tasks, for now we will just download 1000 posts from our dataset (or fewer, if your subreddit has fewer than 1000 posts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signing up to use the Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sign up\n",
    "\n",
    "Go to http://www.reddit.com and **sign up**\n",
    "\n",
    "### 2. Create an app\n",
    "Go to https://ssl.reddit.com/prefs/apps/ and click on `create app`:\n",
    "\n",
    "<img style=\"width: 1000px; height: 361px; float: right; margin: 2 2 2 2;\" src=\"img/reddit-1.png\" />\n",
    "\n",
    "### 3. Note details \n",
    "Note the client ID, client secret, and your username/password for Reddit:\n",
    "\n",
    "<img style=\"width: 1000px; height: 395px; float: right; margin: 2 2 2 2;\" src=\"img/reddit-2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Reddit API\n",
    "\n",
    "With the details we just created, we can access the Reddit API using PRAW [Python Reddit API Wrapper].\n",
    "We're downloading 1000 posts and their associated comments.\n",
    "\n",
    "For the purpose of this exercise, we'll download them in one data file, but it's common practice to download posts and comments in two different relational databases (can you think of why this is?)\n",
    "\n",
    "First, we enter the user details of the app we just created. Then, we run a function that retrieves the post and its associated metadata, as well as the comments. We save the information in a CSV.\n",
    "\n",
    "**Note:** you might want to add other metadata elements to your function, or organize it differently (for instance, you could capture the comments separately). For a list of the attibutes you can use, check:\n",
    "\n",
    "* https://praw.readthedocs.io/en/latest/code_overview/models/submission.html for submissions/posts\n",
    "* https://praw.readthedocs.io/en/latest/code_overview/models/comment.html for comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import json\n",
    "import codecs\n",
    "import os \n",
    "\n",
    "maxCount = 1000  # change depending on how much data you need (max is 1000)\n",
    "\n",
    "def main():\n",
    "    \"\"\"This uses PRAW, a Python package for using the Reddit API, in order to create an access \n",
    "    token to scrape data from Reddit.\"\"\"\n",
    "    # Change the name of this variable to your preferred filename\n",
    "    fileName = \"SUBREDDIT_NAME_HERE\" + \"_\" + str(maxCount) + \"_\" + datetime.now().strftime('%Y%m%d') + \".csv\"\n",
    "    writer = csv.writer(open(fileName, 'wt', encoding = 'utf-8'))\n",
    "    writer.writerow(['no.', 'url', 'date', 'author', 'score', 'flair', 'num_comments', 'title', 'body', 'comments'])   \n",
    "    # Change the name of these variables to those of your Reddit app\n",
    "    reddit = praw.Reddit(client_id='CLIENT_ID_HERE',\n",
    "                     client_secret='CLIENT_SECRET_HERE',\n",
    "                     password='REDDIT_PSW_HERE',\n",
    "                     user_agent='reddit_posts',\n",
    "                     username='REDDIT_USERNAME_HERE'\n",
    "                     )\n",
    "    print(\"Retrieving data...\", end=\"\", flush=True)\n",
    "    get_data(reddit, writer)\n",
    "    print(\"Done!\" + \"\\n\" + \"Found \" + str(itemCount) + \" posts\" + \"\\n\" + \"Found \" + str(commentCount) + \" comments\")\n",
    "\n",
    "def get_data(reddit, writer):\n",
    "    global itemCount \n",
    "    itemCount = 0\n",
    "    global commentCount \n",
    "    commentCount = 0\n",
    "    params = {'sort':'new', 'limit':None, 'syntax':'lucene'}\n",
    "    # Change the name of this variable to your preferred subreddit name (e.g. \"changemymind\")\n",
    "    for submission in reddit.subreddit('SUBREDDIT_NAME_HERE').top(limit=None): \n",
    "    # limit=None sets max to 1000. \n",
    "    # Instead of .top you can also try .hot, .controversial, or .search('SEARCHTERM', **params)\n",
    "    # E.g. for submission in reddit.subreddit('amitheasshole').search('flair:\"YTA\"', **params):\n",
    "        itemCount += 1\n",
    "        timestamp = submission.created\n",
    "        date = datetime.fromtimestamp(timestamp).strftime('%Y' + '-' + '%m' + '-' + '%d')\n",
    "        title = submission.title\n",
    "        url = submission.url\n",
    "        body = submission.selftext\n",
    "        author = submission.author\n",
    "        flair = submission.link_flair_text\n",
    "        score = submission.score\n",
    "        num_comments = submission.num_comments\n",
    "        commentList = []\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            if comment.author != None:\n",
    "                commentCount += 1\n",
    "                commentList.append(comment.body)\n",
    "        comments = ' '.join(commentList)\n",
    "        writer.writerow( (itemCount, url, date, author, score, flair, num_comments, title, body, comments) )\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        if itemCount == maxCount:\n",
    "            break\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
