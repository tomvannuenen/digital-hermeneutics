{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Week 3-3 topic modeling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DJBqAZKY_PF",
        "colab_type": "text"
      },
      "source": [
        "<div style=\"display: block; width: 100%; height: 120px;\">\n",
        "\n",
        "<p style=\"float: left;\">\n",
        "    <span style=\"font-weight: bold; line-height: 24px; font-size: 16px;\">\n",
        "        DIGHUM160 - Critical Digital Humanities\n",
        "        <br />\n",
        "        Digital Hermeneutics 2019\n",
        "    </span>\n",
        "    <br >\n",
        "    <span style=\"line-height: 22x; font-size: 14x; margin-top: 10px;\">\n",
        "        Week 3-3: Topic modeling <br />\n",
        "        Created by Tom van Nuenen (tom.van_nuenen@kcl.ac.uk)\n",
        "    </span>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwZnlfT4Y_PF",
        "colab_type": "text"
      },
      "source": [
        "# Topic modeling\n",
        "\n",
        "Topic modeling is a type of statistical modeling for the discovery of abstract \"topics\" that occur in a collection of documents. It is frequently used in NLP to aid the discovery of hidden semantic structures in a collection of texts.\n",
        "\n",
        "By the end of this notebook, you should:\n",
        "\n",
        "* have practiced with creating and visualising topic models using Scikit-LEARN;\n",
        "* have practiced with close reading top documents associated with topics of interest.\n",
        "\n",
        "There are lots of Python packages for topic modeling. We will use Scikit-LEARN. \n",
        "If you have access to your Reddit dataset, feel free to use that instead.\n",
        "\n",
        "The example dataset once again comes from the banned subreddit \"The Red Pill\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aqdiMtW1GKZ",
        "colab_type": "text"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbMEysjJY_PI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction import text\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from more_itertools import chunked\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import brown\n",
        "!pip install pyLDAvis\n",
        "import pyLDAvis.sklearn \n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AVjYueVY_PM",
        "colab_type": "text"
      },
      "source": [
        "## Importing data\n",
        "\n",
        "Our (abridged) DataFrame includes 10,000 posts and 10,000 comments, sorted by the highest score. We have two files: one for the submissions, and one for the comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjxJwncSo4_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81Pk2xVGoMoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1hN5eqCYVZOX_O0i8waJUQxDlJjqDMenK\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('TRP_submissions.csv')       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYUfVU8xp3eN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1nY9JtXoGJa7B-OmU6afh4qfcFGPCQIHW\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('TRP_comments.csv')        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jfT6806Y_PM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trp_sub = pd.read_csv(\"TRP_submissions.csv\", lineterminator='\\n')\n",
        "trp_com = pd.read_csv(\"TRP_comments.csv\", lineterminator='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzzk4IdzY_PP",
        "colab_type": "text"
      },
      "source": [
        "Let's have a quick look at the comments:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "O6Sh-o1FY_PQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trp_com.sort_values(by=['score'], ascending=False).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ked1C5d8rj-C",
        "colab_type": "text"
      },
      "source": [
        "How many comments do we have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwZiIuAurjhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(trp_com)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lVl1xtHY_PU",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing\n",
        "Let's have a look at the text in our `trp_com['body']` column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl7MVNwkxVlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trp_com['body'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooknyCu_xZR7",
        "colab_type": "text"
      },
      "source": [
        "Looks like we have a bit of cleanup to do..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEQW9iqBfnj9",
        "colab_type": "text"
      },
      "source": [
        "### Data cleaning using RegEx\n",
        "Regular Expressions are often used to clean up data – special characters, newlines, and so on. Here we use RegEx to remove newlines and single quotes from our `trp_com['body']` column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiZ572m-fn1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove new line characters\n",
        "trp_com['body'] = [re.sub(r'\\s+', ' ', sent) for sent in trp_com['body']]\n",
        "# Remove distracting single quotes\n",
        "trp_com['body'] = [re.sub(r\"\\'\", \"\", sent) for sent in trp_com['body']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwKHa_y-gRss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trp_com['body'][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dSM0Qr4Y_PV",
        "colab_type": "text"
      },
      "source": [
        "### Getting a slice of data\n",
        "Next, let's create a small list for testing purposes from the texts in one of our DataFrames. We can easily create lists from DataFrame columns using the Pandas `.tolist()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAZs3KX_Y_PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trp_test = trp_com['body'][:10].tolist()\n",
        "trp_test[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nok4Tn4wQQgT",
        "colab_type": "text"
      },
      "source": [
        "### POS tagging & filtering\n",
        "\n",
        "POS refers to \"Part Of Speech\". There are eight parts of speech in the English language: noun, pronoun, verb, adjective, adverb, preposition, conjunction, and interjection. This indicates how the word functions in meaning as well as grammatically within the sentence.\n",
        "\n",
        "NLTK and other libraries allow us to POS tag our tokens. This is pretty straightforwardly done using the `.pos_tag()` method.\n",
        "\n",
        "Instead of creating our topic model from all parts of speech in our, we can limit it to nouns. This is a way to focus on thematic information though it does not capture attitudes towards the theme, or other nuances that may be very important to our analysis. \n",
        "\n",
        "*Make sure that, when you create your own topic models, you think about which kinds of topics you aim to find: if you want attitudes, for instance, you might be more interested in adjectives and adverbs.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gFA7BynQSoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run me to see how it works\n",
        "tokens = [\"that\", \"is\", \"the\", \"most\", \"foul\", \"cruel\", \"and\", \"bad-tempered\", \"rodent\", \"you\", \"ever\", \"set\", \"eyes\", \"on\"]\n",
        "pos = nltk.pos_tag(tokens)     \n",
        "pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqoX-eH3RVkC",
        "colab_type": "text"
      },
      "source": [
        "As you can see, `.pos_tag()` returns a list of tuples, with the word at the 0th index, and a POS tag at the 1st index.\n",
        "\n",
        "This is useful, as we can create a new list which only includes certain tags. Let's focus on nouns for now (they are tagged 'NN') (For a rundown of all the NLTK POS tags, see [here](https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/).)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqdSS81cRUeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nouns = []\n",
        "for tup in pos:   \n",
        "  if tup[1] == \"NN\":\n",
        "    nouns.append(tup[0])\n",
        "nouns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGHWp01NRXCT",
        "colab_type": "text"
      },
      "source": [
        "As you see, POS tagging is not perfect.\n",
        "\n",
        "Your turn! Create a function called `pos_tagger`. It takes one parameter called `text`, which is a string.\n",
        "1. Use `re.sub` to get rid of newlines and special characters; \n",
        "2. Use `.word_tokenize` to tokenize the string'; \n",
        "3. Use `nlkt.pos_tag` to tag the tokens;\n",
        "4. Put only the nouns (so *not* the tags!) in a new list;\n",
        "5. `return` that list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W7cKAUER_Rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKxU66YluIVB",
        "colab_type": "text"
      },
      "source": [
        "Now, run your function over the `trp_test` list we just created, and print out some nouns to see if it worked (remember to run your function in a `for`-loop!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWYpdDftuCVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trp_nouns = [pos_tagger(each) for each in trp_test]\n",
        "trp_nouns[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2NudH9oYa94",
        "colab_type": "text"
      },
      "source": [
        "Now, let's apply function to our entire DataFrame so that you can work with either the cleaned-up or the original text. This is generally good practice.\n",
        "\n",
        "1. Create a new column in our `trp_com` DataFrame, called \"body_nouns\";\n",
        "2. Assign it to the output of your `pos_tagger` function, which you loop over each comment in `trp_com['body']`. Make sure to use `' '.join()` on the output of the `pos_tagger` first, so that you get a string.\n",
        "\n",
        "Have a look at your DataFrame once you're done to see if you succeeded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu68d3tyYsbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRs4mMHIY_PY",
        "colab_type": "text"
      },
      "source": [
        "## Topic modeling\n",
        "Time to build our topic model! Before we do so, we need to turn our corpus into word counts.\n",
        "\n",
        "### Using `CountVectorizer`\n",
        "We use the Scikit-LEARN's `CountVectorizer` from last week's tf-idf exercise again – but this time, we only look at term frequencies. What this results in is a matrix of (almost) the entire vocabulary within our corpus, and the counts of these words. \n",
        "\n",
        "Note that we set the `max_features` to 1000, which means we only use the top-1000 words in terms of TF. We also remove words that don't occur more than twice (`min_df=2`), and words that occur in more than 95% of the documents (`max_df=0.95`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYE6DcbBY_PZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We're only training for 1000 features (i.e., most-occurring words) Feel free to change this.\n",
        "no_features = 1000\n",
        "\n",
        "# Using TF vectorizer to get top terms\n",
        "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
        "tf = tf_vectorizer.fit_transform(trp_com['body_nouns'])\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21AF8sJEscWX",
        "colab_type": "text"
      },
      "source": [
        "Note that `tf_feature_names`, which we got through Scikit-LEARN's `.get_feature_names()` method, is just a list of the words in our 1000-word vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeudrqLCsp7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(tf_feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w7E2nk8sVD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_feature_names[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbAttlpI-jL7",
        "colab_type": "text"
      },
      "source": [
        "### Topic modeling using `LatentDirichletAllocation` \n",
        "\n",
        "Next, we run scikit-LEARN's `LatentDirichletAllocation` class. Note that we can choose how many topics we want to find – by far the most important parameter to set when creating a topic model. Let's start with 10.\n",
        "\n",
        "Some other parameters to understand:\n",
        "- `max_iter` determines the maximum number of iterations to be performed when fitting the model.\n",
        "- Setting `random_state` to 0 controls the random number generator used by Scikit-LEARN. This results in reproducible topics.\n",
        "\n",
        "For more info, see [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9qoJXgf-ivu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We're only training for 10 topics in our topic model (feel free to change this)\n",
        "no_topics = 10\n",
        "\n",
        "# Run LDA\n",
        "lda_model = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
        "lda_W = lda_model.transform(tf)\n",
        "lda_H = lda_model.components_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH5-8XAsrOS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(lda_W)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CRFuDxfdiJa",
        "colab_type": "text"
      },
      "source": [
        "Okay, what variables have we created here? \n",
        "- `lda_model` sets up our model and its parameters, after which we apply `.fit(tf)` to *fit* it to our TF matrix; \n",
        "- `LDA_W` is a topics-to-documents matrix (the probability distribution of the topics present in each document, or in our case, comment) - so a list of 10000 elements (the amount of comments we have here).\n",
        "- `LDA_H` is a words-to-topics matrix (the probability distribution of the words belonging to each topic) - so a list of 10 elements (the amount of topics we decided to infer).\n",
        "\n",
        "We can use these two matrices to print out the most significant words for each topic in the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LfTFd9F9Q5F",
        "colab_type": "text"
      },
      "source": [
        "### Displaying the topics\n",
        "\n",
        "So we have a topic model. But how to display it?\n",
        "We'll write a `display_topics()` function, which takes both the words-to-topics matrix (`H`) and the `feature_names` as parameters.\n",
        "\n",
        "Our `display_topics` function prints out a numerical index as the topic name, and prints the top words in the topic. Numpy's `argsort()` method is used to sort the row or column of the matrix: it returns the indexes for the cells that have the highest weights in order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo4Ig6Vn9OvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_topics(H, feature_names,no_top_words):\n",
        "    for topic_idx, topic in enumerate(H):\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-yhkZFo9P5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now print out the top words for each topic\n",
        "no_top_words = 10\n",
        "display_topics(lda_H, tf_feature_names, no_top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP6q-KTNtOiv",
        "colab_type": "text"
      },
      "source": [
        "Here we have our 10 topics and the most-associated words. While these topics are probably not very accurate (we used only a very small dataset), we could derive some insights from this already: for instance, topic 8 seems to be about posts in which members of The Red Pill discuss how men are turned into \"betas\" through the force of feminism. \n",
        "\n",
        "You can see that, in order to make sense of the topics you create, you have to understand the lingo and logic of a particular community – hence the annotations you've been doing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSdQIVqnY_Pd",
        "colab_type": "text"
      },
      "source": [
        "### Retrieving top documents per topic\n",
        "\n",
        "The output of our `display_topics` function involved assigning a numeric label to the topics and printing out the top words in each topic. This is common practice. However, just displaying the top words in a topic may not help us to understand what each topic is about or determine the *context* in which these words are used.\n",
        "\n",
        "So, let's define a function that gets both the topics and the associated top document.\n",
        "\n",
        "This function now also needs to take the original \"document\" collection (our `trp_com['body']` column) and number of top documents (no_top_documents), as well as the words (feature_names) and number of top words (no_top_words). It then prints the top documents in the topic. The top words and top documents have the highest weights in the returned matrices. \n",
        "\n",
        "The function returns our 10 topics again, but this time also prints the associated top document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ND6sjNRY_Pe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display_topic_docs(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
        "    for topic_idx, topic in enumerate(H):\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
        "        for doc_index in top_doc_indices:\n",
        "            print(documents[doc_index])\n",
        "\n",
        "# We're printing 10 top words per topic, and 3 \"most representative\" documents per topic. Feel free to change this.\n",
        "no_top_words = 10\n",
        "no_top_documents = 3\n",
        "display_topic_docs(lda_H, lda_W, tf_feature_names, trp_com['body'], no_top_words, no_top_documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c9LWNujY_Ph",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing topics using pyLDAvis\n",
        "We can visualize our topic model using pyLDAvis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BcYEUD9zY_Pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pyLDAvis.sklearn \n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.sklearn.prepare(lda_model, tf, tf_vectorizer, mds='tsne')\n",
        "pyLDAvis.save_html(panel, 'lda.html')\n",
        "panel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siC4WDlhY_Pm",
        "colab_type": "text"
      },
      "source": [
        "To the left, you see your topics, represented as bubbles. To the right, you see the top words based on overall term frequency. You can click on the bubbles to see the most-prevalent words within particular topics.\n",
        "\n",
        "Using the λ slider, you can rank the terms according to term relevance. By default, the terms of a topic are ranked in decreasing order according to their topic-specific probability ( λ = 1 ). Moving the slider allows you to adjust the rank of terms based on how discriminatory (or \"relevant\") they are for the specific topic. The suggested “optimal” value of λ is 0.6.\n",
        "\n",
        "*Note: a \"good\" topic model will have non-overlapping, fairly big sized blobs for each topic.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioCRLwV5Y_Pm",
        "colab_type": "text"
      },
      "source": [
        "**OPTIONAL CHALLENGE: sorting data by thread!**\n",
        "\n",
        "If you have time left, have another look at your original two DataFrames. Try to think of a way to use the IDs of both comments and submissions to create a list in which the original submission and comments are put together. Tip: look into the `pd.merge()` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_ZTIIUeY_Pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSqrsqUoY_Pq",
        "colab_type": "text"
      },
      "source": [
        "## Exercise: from distant to close reading\n",
        "\n",
        "- Play around with the amount of topics, and the amount of tokens being processed per \"document\";\n",
        "- Try to locate interesting topics, and their associated top documents;\n",
        "- Close read the top documents associated with your topic of interest, paying attention to concerns we discussed this week:\n",
        "    - Formal aspects; e.g. emotive VS rationalist language, specialised lexicon\n",
        "    - Russian Formalists: *defamiliarization*\n",
        "    - Derrida: *breaking down implied dichotomies*\n",
        "    - Barthes: *myth-making*\n",
        "    \n",
        "- **If you've been able to download your own Reddit dataset, try to use that data!**"
      ]
    }
  ]
}